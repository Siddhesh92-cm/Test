you or making it too complex,

first we will ask vlm/llm to extract the data from dashboard right,

then we should extract all the data from dashboard using OCR,

then give data from both vlm/llm and OCR to llm to generate test cases

Yes — that 3‑step flow is the right balance of simplicity and accuracy: extract structure and values with a vision LLM, run a CPU OCR pass for precise text capture, then feed both artifacts to the LLM to generate BI‑grade test cases. [1][2][3]

### Plan
- Step 1 (VLM extract): Use the OpenAI Responses API to read the screenshot and output compact JSON describing charts, KPIs, filters/parameters, interactions, and approximate chart tables. [1][3]
- Step 2 (OCR extract): Run a CPU OCR pass for high‑recall text spans and tiny ticks; use PaddleOCR locally with no extra keys, or keep it OpenAI‑only by prompting the vision model for OCR if you prefer a single dependency. [2][1]
- Step 3 (Generate tests): Provide the image + both JSON artifacts to the OpenAI model, and instruct it to use OCR text for exact strings and numbers while using VLM roles/structure for semantics, grounded on Power BI/Tableau QA checklists. [4][5]

### Minimal end‑to‑end script (OpenAI + optional PaddleOCR)
```python
# file: simple_dash_qa.py
import os, json, base64, argparse, uuid
from typing import List, Dict, Any
from openai import OpenAI

# Optional OCR on CPU (no extra keys). Set USE_PADDLE=1 to enable.
USE_PADDLE = os.getenv("USE_PADDLE", "0") == "1"
if USE_PADDLE:
    from paddleocr import PaddleOCR

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_PRIMARY = os.getenv("OPENAI_MODEL", "o4-mini")
MODEL_FALLBACK = os.getenv("OPENAI_MODEL_FALLBACK", "o3")

def b64(path: str) -> str:
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

def call_responses(client: OpenAI, model: str, content: List[Dict[str, Any]]) -> str:
    resp = client.responses.create(model=model, input=[{"role": "user", "content": content}])
    return resp.output_text

# Step 1: VLM extraction (structure + rough values)
def extract_vlm(client: OpenAI, img_b64: str) -> Dict[str, Any]:
    prompt = (
        "From this dashboard image, extract a compact JSON with keys:\n"
        "layout: {charts[], kpis[], filters[], parameters[], interactions[]}, and\n"
        "chart_tables: [{type,title,legend[],axes:{x_labels[],y_label},series:[{name,values[]}]}].\n"
        "Each element should include bbox:[x1,y1,x2,y2] if inferable (normalized 0..1). "
        "Return JSON only."
    )
    content = [
        {"type": "input_text", "text": prompt},
        {"type": "input_image", "image_url": f"data:image/png;base64,{img_b64}"},
        {"type": "input_text", "text": "JSON only."},
    ]
    try:
        return json.loads(call_responses(client, MODEL_PRIMARY, content))
    except Exception:
        try:
            return json.loads(call_responses(client, MODEL_FALLBACK, content))
        except Exception:
            return {"layout": {"charts": [], "kpis": [], "filters": [], "parameters": [], "interactions": []}, "chart_tables": []}

# Step 2a: OpenAI OCR (single dependency)
def extract_ocr_openai(client: OpenAI, img_b64: str) -> List[Dict[str, Any]]:
    prompt = (
        "Perform OCR on this dashboard image and return a JSON array of items: "
        "{text, bbox:{x1,y1,x2,y2} normalized 0..1, score in [0,1], role one of "
        "[title, legend, axis_label, tick, kpi_title, kpi_value, filter_label, filter_value, note, other]}. "
        "JSON only."
    )
    content = [
        {"type": "input_text", "text": prompt},
        {"type": "input_image", "image_url": f"data:image/png;base64,{img_b64}"},
        {"type": "input_text", "text": "JSON only."},
    ]
    try:
        return json.loads(call_responses(client, MODEL_PRIMARY, content))
    except Exception:
        try:
            return json.loads(call_responses(client, MODEL_FALLBACK, content))
        except Exception:
            return []

# Step 2b: PaddleOCR OCR (optional, CPU)
_paddle = None
def extract_ocr_paddle(image_path: str) -> List[Dict[str, Any]]:
    global _paddle
    if _paddle is None:
        _paddle = PaddleOCR(lang="en", use_angle_cls=True)  # CPU by default
    out = []
    for page in _paddle.ocr(image_path, cls=True):
        for box, (text, score) in page:
            xs = [p[0] for p in box]; ys = [p[1] for p in box]
            x1, y1, x2, y2 = min(xs), min(ys), max(xs), max(ys)
            out.append({"text": text, "bbox_px": [float(x1), float(y1), float(x2), float(y2)], "score": float(score), "role": "other"})
    return out

# Step 3: Generate test cases using both extracts
def generate_tests(client: OpenAI, img_b64: str, vlm_json: Dict[str, Any], ocr_json: List[Dict[str, Any]], dash_name: str) -> List[Dict[str, Any]]:
    prompt = (
        f"You are a QA assistant for BI dashboard '{dash_name}'. "
        "Use VLM layout/values for semantics, and OCR text for exact strings and numbers. "
        "Generate test cases covering categories: accuracy, filters, interaction, performance, visual. "
        "Each case fields: id, category, preconditions, steps, expected, evidence, automation, data_needed. "
        "Ground coverage on Power BI 'validate content' and Tableau dashboard testing/performance checklists. "
        "Return a JSON array only."
    )
    content = [
        {"type": "input_text", "text": prompt},
        {"type": "input_text", "text": "VLM JSON:"},
        {"type": "input_text", "text": json.dumps(vlm_json, ensure_ascii=False)},
        {"type": "input_text", "text": "OCR JSON:"},
        {"type": "input_text", "text": json.dumps(ocr_json, ensure_ascii=False)},
        {"type": "input_text", "text": "Dashboard image:"},
        {"type": "input_image", "image_url": f"data:image/png;base64,{img_b64}"},
        {"type": "input_text", "text": "Output only the JSON array of test cases."},
    ]
    try:
        return json.loads(call_responses(client, MODEL_PRIMARY, content))
    except Exception:
        try:
            return json.loads(call_responses(client, MODEL_FALLBACK, content))
        except Exception:
            return [{
                "id": f"TC-{uuid.uuid4().hex[:8]}",
                "category": "visual",
                "preconditions": [],
                "steps": ["Open dashboard and check titles/labels/KPI values."],
                "expected": ["All text is legible and matches design."],
                "evidence": ["screenshot"],
                "automation": False,
                "data_needed": {},
            }]

def run(image_path: str, out_path: str, dash_name: str):
    client = OpenAI(api_key=OPENAI_API_KEY)
    img_b64 = b64(image_path)

    vlm_json = extract_vlm(client, img_b64)
    ocr_json = extract_ocr_paddle(image_path) if USE_PADDLE else extract_ocr_openai(client, img_b64)
    cases = generate_tests(client, img_b64, vlm_json, ocr_json, dash_name)

    result = {"dashboard_name": dash_name, "source_image": image_path, "vlm_json": vlm_json, "ocr_json": ocr_json, "cases": cases}
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"Wrote {out_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Path to dashboard image")
    parser.add_argument("--out", required=True, help="Path to write *.tests.json")
    parser.add_argument("--name", default="Dashboard", help="Dashboard name")
    args = parser.parse_args()
    assert OPENAI_API_KEY, "Set OPENAI_API_KEY"
    run(args.input, args.out, args.name)
```

### How to run
- OpenAI‑only: install just the OpenAI SDK, set OPENAI_API_KEY, and run; both VLM extraction and OCR happen in the cloud via image+text prompts using the Responses API. [1][3]
- With CPU OCR: also install PaddleOCR and set USE_PADDLE=1 to enable the local OCR pass for best recall on tiny text. [2]

Commands:
- OpenAI‑only: pip install "openai>=1.40.0" && export OPENAI_API_KEY=sk-... && python simple_dash_qa.py --input ./dash.png --out ./dash.tests.json [3][1]
- With PaddleOCR: pip install "paddleocr[all]" && USE_PADDLE=1 python simple_dash_qa.py --input ./dash.png --out ./dash.tests.json [2]

### Prompting and coverage
- The prompts instruct JSON‑only outputs and explicitly request categories aligned with Microsoft’s Power BI content validation and Tableau dashboard testing/performance best practices, which drives consistent, pass/fail‑ready cases. [4][5]
- Using OpenAI o4‑mini as default keeps latency and cost reasonable while supporting multimodal reasoning; switch to o3 via environment variable for harder dashboards. [6][3]

### Why this stays simple
- Three small calls: one for VLM layout/values, one for OCR (OpenAI‑only or PaddleOCR), and one to generate test cases, with no GPUs or extra paid services required. [1][2][3]

Citations:
[1] OpenAI's image vision guide https://platform.openai.com/docs/guides/images-vision?api-mode=responses
[2] Quick Start - PaddleOCR Documentation https://paddlepaddle.github.io/PaddleOCR/main/en/quick_start.html
[3] Responses API reference https://platform.openai.com/docs/api-reference/responses
[4] Power BI implementation planning: Validate content https://learn.microsoft.com/en-us/power-bi/guidance/powerbi-implementation-planning-content-lifecycle-management-validate
[5] Tableau Dashboard Testing Checklist https://www.datagaps.com/blog/tableau-dashboard-testing-checklist/
[6] OpenAI Models Documentation https://platform.openai.com/docs/models
