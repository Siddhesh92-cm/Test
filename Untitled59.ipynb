Updated implementation without recurssion limit,
 async def run_agent_and_collect(agent: Any, sub question: str)

-> str:

Run the agent asynchronously and collect the latest AI message content.

all chunks - []

async for chunk in agent.astream

("messages": [{"role": "user", "content": sub_question}]},

stream_node "updates",

all chunks.append(chunk)

logger.info(f"Agent step chunk: (chunk)")

Look for the final response in different ways

latest content - None

Method 1: Look for agent messages with substantial content

for chunk in all chunks:

agent data chunk.get("agent", ())

Check agent messages

messages agent data.get("messages", []) if isinstance(agent data, dict) else []

for asg in messages:

content getattr(msg, "content", None) or (

msg.get("content") if isinstance(asg, dict) else None

if content and len(content.strip()) > 50:

Increased threshold for substantial content

latest content content

logger.debug("Found agent message content: [content[:100]}}...")

Method 2: Look for output content

for chunk in all chunks:

output-chunk.get("output")

If output:

content getattr(output, "content", None) or ( output.get("content") if isinstance(output, dict) else Hone

If content and len(content.strip()) > 50:

logger.debug("Found output content: (content[:100])...")

#Method 3: Look for any messages with good content

for chunk in all_chunks:

messages chunk.get("messages", [])

if isinstance(messages, list):

for msg in messages:

content getattr(msg, "content", None) or (

) msg.get("content") if isinstance(msg, dict) else None

if content and len(content.strip()) > 50:

latest_content content

logger.debug("Found message content: (content[:100])...")

Method 4: Check for any chunk with substantial text content

if not latest_content:

if isinstance(chunk, dict):

for chunk in all_chunks:

for key, value in chunk.items():

if isinstance(value, str) and len(value.strip()) > 100:

latest content = value

logger.debug("Found chunk content in (key): (value[:100]}...") break

#Log what we found

if latest content:

logger.info(f Successfully extracted content with (len(latest_content)) characters") else

: logger.warning("No substantial content found in agent execution")

return latest_content or "No information found about this clinical trial."

async def research_sub_query(

sub_question_data: Dict[str, Any],

original_query: str,

refined query: str,

server_type: ServerType,

)-> Dict[str, Any]:

Research a single sub-question using appropriate tools.

Args:

sub_question data (Dict(str, Any]): Sub-question data including question and potential_tools

original query (str): Original user query

refined query (str): Refined query from previous node

Returns:

Dict(str, Any]: Research results including the answer and tool usage

sub_question sub_question_data.get("question", "")

tool names sub question_data.get("potential_tools", [])

priority sub_question_data.get("priority", 1)

logger.info(f"Researching sub-query (priority (priority)): (sub_question)")

logger.info("Using tools: (tool_names)")

try:

Create a simple variable to store the last Al message content This is our absolute fallback to ensure we always have something If ToolSelector[server_type]["data_type"] = "MCP":

server_url ToolSelector[server_type]["tools"]

If not server_url:

logger.error( "No MCP server URL provided and no default URL configured." return "Error: No KCP server URL available."

logger.info("Connecting to MCP server at: (server_url]")

async with streamablehttp_client (server url) as (

client_read stream, client write stream, client get_session_id_callback

async with ClientSession(client_read_stream, client write strean) as

scp session:

session: ClientSession acp session try :

Iogger.debug("Attempting session. Initialize()...")

avait session.Initialize()

logger.debug("session.initialize() call completed without raising Exception.")

except Exception as init error:

logger.error("Exception caught directly during session.initialize(): (type(Init_error) name) (init_error)") raise

all tools avait load_scp_tools(session)

Usa all available tools instead of filtering by tool names

this allows the agent to use any relevant tools for research

tools all tools

Ingger.info("Loaded (len(tools)) tools from ICP server: ([tool.name for tool in tools)))


context "Original query: (original_query}\nRefined query: {refined_query}\n"

agent ) create_research_agent( tools-tools, sub_question-sub_question, context context, server_type-server_type,

answer await run_agent_and_collect(agent, sub_question)

else:

For NON-MCP, you may need to implement actual tool loading logic

all_tools ToolSelector[server_type] ["tools"]

Use all available tools for non-MCP as well

tools all_tools

context(

) f"Original query: (original_query)\nRefined query: (refined_query}\n"

agent create research_agent( ) tools-tools, sub_question-sub_question, context-context, server_type-server_type,

answer await run_agent_and_collect(agent, sub_question)

Log the final answer we're storing

logger.info(

**** FINAL ANSWER BEING STORED: (answer:200]}..." if len(answer) > 200
üëç
else f*** FINAL ANSWER BEING STORED: (answer)"
)

return {"sub_question": sub_question, "answer": answer}

except Exception as e:

logger.error(f"Error researching sub-question {sub_question)': {e}") answer = f"Error researching this question: {str(e)}" "sub_question": sub_question, "answer": f"Error researching this question: (str(e)}",

}
