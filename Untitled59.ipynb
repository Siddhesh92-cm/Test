{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import hashlib\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "# AWS Clients\n",
    "s3 = boto3.client('s3')\n",
    "secrets = boto3.client('secretsmanager')\n",
    "dynamodb = boto3.resource('dynamodb').Table('extraction_state')\n",
    "\n",
    "# Constants\n",
    "BUCKET = 'your-data-bucket'\n",
    "SOURCES = ['0_0', '0_1', 'ctgov']\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    try:\n",
    "        return secrets.get_secret_value(SecretId=secret_name)['SecretString']\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Secret {secret_name} retrieval failed: {str(e)}\")\n",
    "\n",
    "def check_existing_extraction(source, year_month):\n",
    "    response = dynamodb.get_item(Key={'source': source, 'year_month': year_month})\n",
    "    return 'Item' in response\n",
    "\n",
    "@retry(wait=wait_exponential(), stop=stop_after_attempt(3))\n",
    "def fetch_0_0(api_key, year_month):\n",
    "    \"\"\"Fetch 0_0 articles for a specific month\"\"\"\n",
    "    articles = []\n",
    "    query = f\"{datetime.strptime(year_month, '%Y-%m').strftime('%Y/%m')}[dp]\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    \n",
    "    search_url = f\"{base_url}esearch.fcgi?db=0_0&term={query}&api_key={api_key}&retmax=100000\"\n",
    "    search_res = requests.get(search_url)\n",
    "    id_list = search_res.text.split('<Id>')[1:-1]\n",
    "    \n",
    "    for i in range(0, len(id_list), 500):\n",
    "        batch_ids = [id.split('<')[0] for id in id_list[i:i+500]]\n",
    "        fetch_url = f\"{base_url}efetch.fcgi?db=0_0&id={','.join(batch_ids)}&api_key={api_key}&retmode=xml\"\n",
    "        fetch_res = requests.get(fetch_url)\n",
    "        articles.extend(fetch_res.text.split('<0_0Article>')[1:])\n",
    "    \n",
    "    return articles\n",
    "\n",
    "@retry(wait=wait_exponential(), stop=stop_after_attempt(3))\n",
    "def fetch_ctgov(year_month):\n",
    "    \"\"\"Fetch 0_2 trials for a specific month\"\"\"\n",
    "    trials = []\n",
    "    page_size = 100\n",
    "    skip = 0\n",
    "    \n",
    "    while True:\n",
    "        url = f\"https://clinicaltrials.gov/api/query/full_studies?expr=AREA[LastUpdatePostDate]RANGE[{year_month}-01,{year_month}-31]&min_rnk={skip+1}&max_rnk={skip+page_size}&fmt=json\"\n",
    "        res = requests.get(url).json()\n",
    "        trials.extend(res.get(\"FullStudiesResponse\", {}).get(\"FullStudies\", []))\n",
    "        \n",
    "        if len(res.get(\"FullStudiesResponse\", {}).get(\"FullStudies\", [])) < page_size:\n",
    "            break\n",
    "        skip += page_size\n",
    "    \n",
    "    return trials\n",
    "\n",
    "@retry(wait=wait_exponential(), stop=stop_after_attempt(3))\n",
    "def fetch_0_1(api_key):\n",
    "    \"\"\"Fetch 0_1 data\"\"\"\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    response = requests.get(\"https://api.0_1.com/v1/drugs?limit=1000\", headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def process_source(source, year_month):\n",
    "    metadata = []\n",
    "    print(f\"Processing {source} for {year_month}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if already processed\n",
    "        if check_existing_extraction(source, year_month):\n",
    "            print(f\"Already processed {source} for {year_month}\")\n",
    "            return metadata\n",
    "\n",
    "        # Source-specific processing\n",
    "        if source == '0_0':\n",
    "            api_key = get_secret('0_0/api_key')\n",
    "            articles = fetch_0_0(api_key, year_month)\n",
    "            \n",
    "            for article in articles:\n",
    "                file_content = f\"<0_0Article>{article}</0_0Article>\"\n",
    "                pmid = article.split('<PMID Version=\"1\">')[1].split('<')[0]\n",
    "                metadata.append(create_metadata_entry(source, pmid, 'xml', file_content, year_month))\n",
    "\n",
    "        elif source == 'ctgov':\n",
    "            trials = fetch_ctgov(year_month)\n",
    "            \n",
    "            for trial in trials:\n",
    "                nct_id = trial['Study']['ProtocolSection']['IdentificationModule']['NCTId']\n",
    "                file_content = json.dumps(trial)\n",
    "                metadata.append(create_metadata_entry(source, nct_id, 'json', file_content, year_month))\n",
    "\n",
    "        elif source == '0_1':\n",
    "            api_key = get_secret('0_1/api_key')\n",
    "            drugs = fetch_0_1(api_key)\n",
    "            \n",
    "            for drug in drugs.get('results', []):\n",
    "                drug_id = drug['drugId']\n",
    "                file_content = json.dumps(drug)\n",
    "                metadata.append(create_metadata_entry(source, drug_id, 'json', file_content, year_month))\n",
    "\n",
    "        # Mark as processed in DynamoDB\n",
    "        dynamodb.put_item(Item={\n",
    "            'source': source,\n",
    "            'year_month': year_month,\n",
    "            'processed_at': datetime.utcnow().isoformat()\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def create_metadata_entry(source, source_id, file_type, content, year_month):\n",
    "    file_name = f\"{source_id}.{file_type}\"\n",
    "    checksum = hashlib.md5(content.encode()).hexdigest()\n",
    "    s3_path = f\"raw/{source}/{year_month.replace('-', '/')}/{file_name}\"\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET,\n",
    "        Key=s3_path,\n",
    "        Body=content,\n",
    "        Metadata={'source': source}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'file_id': str(uuid.uuid4()),\n",
    "        'source_system': source,\n",
    "        'source_id': source_id,\n",
    "        'file_name': file_name,\n",
    "        'extraction_date': datetime.utcnow().isoformat(),\n",
    "        's3_raw_path': s3_path,\n",
    "        'checksum': checksum,\n",
    "        'file_size': len(content),\n",
    "        'transformation_required': True,\n",
    "        'transformation_status': 'PENDING',\n",
    "        'transformed_s3_path': '',\n",
    "        'last_updated': datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    current_month = datetime.utcnow().strftime('%Y-%m')\n",
    "    all_metadata = []\n",
    "    \n",
    "    for source in SOURCES:\n",
    "        try:\n",
    "            all_metadata.extend(process_source(source, current_month))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if all_metadata:\n",
    "        # Save metadata\n",
    "        df = pd.DataFrame(all_metadata)\n",
    "        metadata_key = f\"metadata/{current_month}/metadata_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}.csv\"\n",
    "        s3.put_object(Bucket=BUCKET, Key=metadata_key, Body=df.to_csv(index=False))\n",
    "        \n",
    "        # Update latest pointer\n",
    "        s3.put_object(Bucket=BUCKET, Key=\"metadata/latest.csv\", Body=df.to_csv(index=False))\n",
    "        \n",
    "        # Trigger Lambda if needed\n",
    "        if df['transformation_required'].any():\n",
    "            lambda_client = boto3.client('lambda')\n",
    "            lambda_client.invoke(\n",
    "                FunctionName='data-transformation-lambda',\n",
    "                InvocationType='Event',\n",
    "                Payload=json.dumps({'metadata_key': metadata_key})\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c02ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lambda\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET = 'your-data-bucket'\n",
    "\n",
    "def transform_0_0(content):\n",
    "    \"\"\"Example 0_0 XML to CSV transformation\"\"\"\n",
    "    from xml.etree import ElementTree as ET\n",
    "    root = ET.fromstring(f\"<root>{content}</root>\")\n",
    "    return {\n",
    "        'pmid': root.findtext('.//PMID'),\n",
    "        'title': root.findtext('.//ArticleTitle'),\n",
    "        'abstract': root.findtext('.//AbstractText')\n",
    "    }\n",
    "\n",
    "def transform_ctgov(content):\n",
    "    \"\"\"Example 0_2 JSON flattening\"\"\"\n",
    "    trial = json.loads(content)\n",
    "    return {\n",
    "        'nct_id': trial['Study']['ProtocolSection']['IdentificationModule']['NCTId'],\n",
    "        'title': trial['Study']['ProtocolSection']['IdentificationModule']['OfficialTitle'],\n",
    "        'phase': trial['Study']['ProtocolSection']['DesignModule'].get('Phase', 'N/A')\n",
    "    }\n",
    "\n",
    "def transform_0_1(content):\n",
    "    \"\"\"Example 0_1 data cleaning\"\"\"\n",
    "    drug = json.loads(content)\n",
    "    return {\n",
    "        'drug_id': drug['drugId'],\n",
    "        'name': drug['name'],\n",
    "        'targets': ', '.join(drug.get('targets', []))\n",
    "    }\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    metadata_key = event['metadata_key']\n",
    "    \n",
    "    try:\n",
    "        # Get metadata\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=metadata_key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        \n",
    "        # Process pending transformations\n",
    "        for index, row in df[(df['transformation_required']) & (df['transformation_status'] == 'PENDING')].iterrows():\n",
    "            try:\n",
    "                # Download raw file\n",
    "                raw_obj = s3.get_object(Bucket=BUCKET, Key=row['s3_raw_path'])\n",
    "                content = raw_obj['Body'].read().decode()\n",
    "                \n",
    "                # Source-specific transformation\n",
    "                if row['source_system'] == '0_0':\n",
    "                    transformed = transform_0_0(content)\n",
    "                elif row['source_system'] == 'ctgov':\n",
    "                    transformed = transform_ctgov(content)\n",
    "                elif row['source_system'] == '0_1':\n",
    "                    transformed = transform_0_1(content)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Upload transformed file\n",
    "                transformed_key = f\"transformed/{row['source_system']}/{row['s3_raw_path'].split('/')[3]}/{row['file_name'].split('.')[0]}.csv\"\n",
    "                csv_data = pd.DataFrame([transformed]).to_csv(index=False)\n",
    "                s3.put_object(Bucket=BUCKET, Key=transformed_key, Body=csv_data)\n",
    "                \n",
    "                # Update metadata\n",
    "                df.at[index, 'transformation_status'] = 'COMPLETED'\n",
    "                df.at[index, 'transformed_s3_path'] = transformed_key\n",
    "                df.at[index, 'last_updated'] = datetime.utcnow().isoformat()\n",
    "                \n",
    "            except Exception as e:\n",
    "                df.at[index, 'transformation_status'] = 'FAILED'\n",
    "                print(f\"Failed to transform {row['file_id']}: {str(e)}\")\n",
    "        \n",
    "        # Save updated metadata\n",
    "        updated_key = metadata_key.replace('/metadata/', '/metadata/processed/')\n",
    "        s3.put_object(Bucket=BUCKET, Key=updated_key, Body=df.to_csv(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return {'statusCode': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bd075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cloudwatch\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import hashlib\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# AWS Clients\n",
    "s3 = boto3.client('s3')\n",
    "secrets = boto3.client('secretsmanager')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Constants\n",
    "BUCKET = os.environ['DATA_BUCKET']\n",
    "SOURCES = ['0_0', '0_1', 'ctgov']\n",
    "METRIC_NAMESPACE = 'DataPipeline'\n",
    "\n",
    "def put_metric(metric_name, value, dimensions):\n",
    "    cloudwatch.put_metric_data(\n",
    "        Namespace=METRIC_NAMESPACE,\n",
    "        MetricData=[{\n",
    "            'MetricName': metric_name,\n",
    "            'Dimensions': dimensions,\n",
    "            'Value': value,\n",
    "            'Unit': 'Count'\n",
    "        }]\n",
    "    )\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    try:\n",
    "        return secrets.get_secret_value(SecretId=secret_name)['SecretString']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Secret {secret_name} retrieval failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry(wait=wait_exponential(), stop=stop_after_attempt(3))\n",
    "def fetch_0_0(api_key, year_month):\n",
    "    \"\"\"Fetch 0_0 articles with enhanced logging\"\"\"\n",
    "    logger.info(f\"Starting 0_0 fetch for {year_month}\")\n",
    "    try:\n",
    "        # ... (keep previous fetch_0_0 implementation)\n",
    "        logger.info(f\"Fetched {len(articles)} 0_0 articles\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        logger.error(f\"0_0 fetch failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_source(source, year_month):\n",
    "    \"\"\"Process a source with CloudWatch metrics and logging\"\"\"\n",
    "    logger.info(f\"Processing source: {source} for {year_month}\")\n",
    "    metadata = []\n",
    "    try:\n",
    "        # Check if metadata already exists for this month\n",
    "        metadata_prefix = f\"metadata/{year_month}/\"\n",
    "        existing = s3.list_objects_v2(Bucket=BUCKET, Prefix=metadata_prefix)\n",
    "        if existing.get('KeyCount', 0) > 0:\n",
    "            logger.warning(f\"Metadata already exists for {year_month}, skipping\")\n",
    "            put_metric('SkippedExtractions', 1, [{'Name': 'Source', 'Value': source}])\n",
    "            return metadata\n",
    "\n",
    "        # ... (keep previous source processing logic)\n",
    "\n",
    "        logger.info(f\"Processed {len(metadata)} files from {source}\")\n",
    "        put_metric('ProcessedFiles', len(metadata), [{'Name': 'Source', 'Value': source}])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed processing {source}: {str(e)}\", exc_info=True)\n",
    "        put_metric('ProcessingErrors', 1, [{'Name': 'Source', 'Value': source}])\n",
    "        raise\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with structured logging\"\"\"\n",
    "    try:\n",
    "        current_month = datetime.utcnow().strftime('%Y-%m')\n",
    "        logger.info(f\"Starting monthly extraction for {current_month}\")\n",
    "        \n",
    "        all_metadata = []\n",
    "        for source in SOURCES:\n",
    "            try:\n",
    "                all_metadata.extend(process_source(source, current_month))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Critical error processing {source}: {str(e)}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        if all_metadata:\n",
    "            # ... (keep previous metadata handling)\n",
    "            logger.info(f\"Uploaded metadata with {len(all_metadata)} entries\")\n",
    "            \n",
    "            # Trigger Lambda\n",
    "            lambda_client = boto3.client('lambda')\n",
    "            lambda_client.invoke(\n",
    "                FunctionName=os.environ['TRANSFORMATION_LAMBDA'],\n",
    "                InvocationType='Event',\n",
    "                Payload=json.dumps({'metadata_key': metadata_key})\n",
    "            )\n",
    "            logger.info(\"Triggered transformation lambda\")\n",
    "\n",
    "        logger.info(\"Extraction completed successfully\")\n",
    "        put_metric('SuccessfulExtractions', 1, [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {str(e)}\", exc_info=True)\n",
    "        put_metric('FailedExtractions', 1, [])\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize logging handler\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lambda cloud\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# AWS Clients\n",
    "s3 = boto3.client('s3')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Constants\n",
    "BUCKET = os.environ['DATA_BUCKET']\n",
    "METRIC_NAMESPACE = 'DataPipeline'\n",
    "\n",
    "def put_metric(metric_name, value, dimensions):\n",
    "    cloudwatch.put_metric_data(\n",
    "        Namespace=METRIC_NAMESPACE,\n",
    "        MetricData=[{\n",
    "            'MetricName': metric_name,\n",
    "            'Dimensions': dimensions,\n",
    "            'Value': value,\n",
    "            'Unit': 'Count'\n",
    "        }]\n",
    "    )\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Lambda handler with enhanced observability\"\"\"\n",
    "    logger.info(\"Starting transformation process\")\n",
    "    metrics = {\n",
    "        'processed': 0,\n",
    "        'success': 0,\n",
    "        'failures': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        metadata_key = event['metadata_key']\n",
    "        logger.info(f\"Processing metadata file: {metadata_key}\")\n",
    "        \n",
    "        # Get metadata\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=metadata_key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        \n",
    "        # Process files\n",
    "        for index, row in df.iterrows():\n",
    "            metrics['processed'] += 1\n",
    "            if not row['transformation_required'] or row['transformation_status'] != 'PENDING':\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # ... (keep previous transformation logic)\n",
    "                metrics['success'] += 1\n",
    "                logger.info(f\"Transformed {row['file_id']} successfully\")\n",
    "                put_metric('TransformationSuccess', 1, [{'Name': 'Source', 'Value': row['source_system']}])\n",
    "                \n",
    "            except Exception as e:\n",
    "                metrics['failures'] += 1\n",
    "                logger.error(f\"Failed to transform {row['file_id']}: {str(e)}\", exc_info=True)\n",
    "                put_metric('TransformationFailures', 1, [{'Name': 'Source', 'Value': row['source_system']}])\n",
    "\n",
    "        # ... (keep metadata update logic)\n",
    "        \n",
    "        logger.info(f\"Transformation complete. Success: {metrics['success']}, Failures: {metrics['failures']}\")\n",
    "        put_metric('TransformationRuns', 1, [{'Name': 'Status', 'Value': 'Completed'}])\n",
    "        return {'statusCode': 200}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical transformation error: {str(e)}\", exc_info=True)\n",
    "        put_metric('TransformationRuns', 1, [{'Name': 'Status', 'Value': 'Failed'}])\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f42a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. CloudWatch Strategy\n",
    "Log Groups:\n",
    "\n",
    "/aws/ecs/extraction-task for ECS logs\n",
    "\n",
    "/aws/lambda/data-transformation for Lambda logs\n",
    "\n",
    "Key Metrics:\n",
    "\n",
    "python\n",
    "Metrics = [\n",
    "    'ProcessedFiles',            # Per source\n",
    "    'SuccessfulExtractions',     # Overall status\n",
    "    'FailedExtractions',\n",
    "    'TransformationSuccess',    # Per source\n",
    "    'TransformationFailures',\n",
    "    'TransformationRuns'         # With Status dimension\n",
    "]\n",
    "Alarms:\n",
    "\n",
    "FailedExtractions > 0 for 1 consecutive period\n",
    "\n",
    "TransformationFailures > 5% of ProcessedFiles for 15 minutes\n",
    "\n",
    "TransformationRuns.Status=Failed > 0 for 1 period\n",
    "\n",
    "Dashboards:\n",
    "\n",
    "Extraction Success Rate: 100 * (SuccessfulExtractions / (SuccessfulExtractions + FailedExtractions))\n",
    "\n",
    "Transformation Failure Rate: 100 * (TransformationFailures / ProcessedFiles)\n",
    "\n",
    "Data Volume Trends: ProcessedFiles by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca43a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infra\n",
    "\n",
    "# Add CloudWatch Alarms\n",
    "transformation_errors_alarm = cloudwatch.Alarm(\n",
    "    self, \"TransformationErrors\",\n",
    "    metric=cloudwatch.Metric(\n",
    "        namespace=METRIC_NAMESPACE,\n",
    "        metric_name=\"TransformationFailures\",\n",
    "        statistic=\"sum\",\n",
    "        period=core.Duration.minutes(5)\n",
    "    ),\n",
    "    threshold=5,\n",
    "    evaluation_periods=1,\n",
    "    comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD\n",
    ")\n",
    "\n",
    "# Add logging permissions\n",
    "lambda_role.add_to_policy(iam.PolicyStatement(\n",
    "    actions=[\n",
    "        \"logs:CreateLogGroup\",\n",
    "        \"logs:CreateLogStream\",\n",
    "        \"logs:PutLogEvents\",\n",
    "        \"cloudwatch:PutMetricData\"\n",
    "    ],\n",
    "    resources=[\"*\"]\n",
    "))\n",
    "\n",
    "ecs_task_role.add_to_policy(iam.PolicyStatement(\n",
    "    actions=[\n",
    "        \"logs:CreateLogStream\",\n",
    "        \"logs:PutLogEvents\",\n",
    "        \"cloudwatch:PutMetricData\"\n",
    "    ],\n",
    "    resources=[\"*\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24970534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data-pipeline/\n",
    "│\n",
    "├── metadata/\n",
    "│   ├── __init__.py\n",
    "│   ├── schema.py\n",
    "│   └── storage.py\n",
    "│\n",
    "├── extraction/\n",
    "│   ├── __init__.py\n",
    "│   ├── extractors.py\n",
    "│   └── task.py\n",
    "│\n",
    "├── transformation/\n",
    "│   ├── __init__.py\n",
    "│   └── lambda_handler.py\n",
    "│\n",
    "├── requirements.txt\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class MetadataEntry:\n",
    "    \"\"\"Data class representing a single file's metadata\"\"\"\n",
    "    source_system: str\n",
    "    source_id: str\n",
    "    file_name: str\n",
    "    extraction_date: datetime\n",
    "    s3_raw_path: str\n",
    "    checksum: str\n",
    "    file_size: int\n",
    "    transformation_required: bool\n",
    "    transformation_status: str = \"PENDING\"\n",
    "    transformed_s3_path: Optional[str] = None\n",
    "    file_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    last_updated: datetime = field(default_factory=datetime.utcnow)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate field constraints\"\"\"\n",
    "        if self.transformation_status not in [\"PENDING\", \"COMPLETED\", \"FAILED\"]:\n",
    "            raise ValueError(f\"Invalid status: {self.transformation_status}\")\n",
    "        if not isinstance(self.transformation_required, bool):\n",
    "            raise TypeError(\"transformation_required must be boolean\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_raw_file(cls, content: bytes, source: str, source_id: str, s3_path: str):\n",
    "        \"\"\"Factory method for creating entries from raw files\"\"\"\n",
    "        return cls(\n",
    "            source_system=source,\n",
    "            source_id=source_id,\n",
    "            file_name=s3_path.split(\"/\")[-1],\n",
    "            extraction_date=datetime.utcnow(),\n",
    "            s3_raw_path=s3_path,\n",
    "            checksum=hashlib.md5(content).hexdigest(),\n",
    "            file_size=len(content),\n",
    "            transformation_required=True\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class MetadataFile:\n",
    "    \"\"\"Collection of metadata entries with file operations\"\"\"\n",
    "    entries: list[MetadataEntry] = field(default_factory=list)\n",
    "\n",
    "    def add_entry(self, entry: MetadataEntry):\n",
    "        entry.validate()\n",
    "        self.entries.append(entry)\n",
    "\n",
    "    def get_pending_transformations(self):\n",
    "        return [entry for entry in self.entries \n",
    "                if entry.transformation_required \n",
    "                and entry.transformation_status == \"PENDING\"]\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        return pd.DataFrame([vars(entry) for entry in self.entries])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame):\n",
    "        return cls([MetadataEntry(**row) for _, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storage module\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetadataStorage:\n",
    "    def __init__(self, bucket_name: str):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.bucket = bucket_name\n",
    "\n",
    "    def save_metadata(self, metadata, year_month: str, versioned=False):\n",
    "        \"\"\"Save metadata to S3 with versioning support\"\"\"\n",
    "        try:\n",
    "            csv_buffer = StringIO()\n",
    "            metadata.to_dataframe().to_csv(csv_buffer, index=False)\n",
    "            \n",
    "            base_key = f\"metadata/{year_month.replace('-', '/')}\"\n",
    "            key = f\"{base_key}/metadata.csv\"\n",
    "            \n",
    "            if versioned:\n",
    "                timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%S')\n",
    "                key = f\"{base_key}/versions/{timestamp}.csv\"\n",
    "\n",
    "            self.s3.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=key,\n",
    "                Body=csv_buffer.getvalue()\n",
    "            )\n",
    "            logger.info(f\"Saved metadata to s3://{self.bucket}/{key}\")\n",
    "            return key\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save metadata: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_metadata(self, key: str):\n",
    "        \"\"\"Load metadata from S3\"\"\"\n",
    "        try:\n",
    "            obj = self.s3.get_object(Bucket=self.bucket, Key=key)\n",
    "            df = pd.read_csv(obj['Body'])\n",
    "            return MetadataFile.from_dataframe(df)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load metadata: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def metadata_exists(self, year_month: str):\n",
    "        \"\"\"Check if metadata exists for given month\"\"\"\n",
    "        try:\n",
    "            prefix = f\"metadata/{year_month.replace('-', '/')}/metadata.csv\"\n",
    "            result = self.s3.list_objects_v2(Bucket=self.bucket, Prefix=prefix)\n",
    "            return result.get('KeyCount', 0) > 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Metadata check failed: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ca82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extractors \n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseExtractor:\n",
    "    def __init__(self, source_name: str):\n",
    "        self.source_name = source_name\n",
    "        \n",
    "    @retry(wait=wait_exponential(), stop=stop_after_attempt(3))\n",
    "    def fetch_data(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class 0_0Extractor(BaseExtractor):\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__(\"0_0\")\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def fetch_data(self, year_month: str):\n",
    "        logger.info(f\"Fetching 0_0 data for {year_month}\")\n",
    "        # Implement 0_0 API logic\n",
    "        return [...]  # List of raw data items\n",
    "\n",
    "class CTGovExtractor(BaseExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ctgov\")\n",
    "\n",
    "    def fetch_data(self, year_month: str):\n",
    "        logger.info(f\"Fetching ClinicalTrials.gov data for {year_month}\")\n",
    "        # Implement 0_2 API logic\n",
    "        return [...]  # List of raw data items\n",
    "\n",
    "class 0_1Extractor(BaseExtractor):\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__(\"0_1\")\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def fetch_data(self, year_month: str):\n",
    "        logger.info(f\"Fetching 0_1 data for {year_month}\")\n",
    "        # Implement 0_1 API logic\n",
    "        return [...]  # List of raw data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction task\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from metadata.schema import MetadataEntry, MetadataFile\n",
    "from metadata.storage import MetadataStorage\n",
    "from extraction.extractors import (\n",
    "    0_0Extractor,\n",
    "    CTGovExtractor,\n",
    "    0_1Extractor\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ExtractionPipeline:\n",
    "    def __init__(self):\n",
    "        self.bucket = os.environ['DATA_BUCKET']\n",
    "        self.storage = MetadataStorage(self.bucket)\n",
    "        self.extractors = {\n",
    "            '0_0': 0_0Extractor(os.environ['0_0_API_KEY']),\n",
    "            'ctgov': CTGovExtractor(),\n",
    "            '0_1': 0_1Extractor(os.environ['0_1_API_KEY'])\n",
    "        }\n",
    "\n",
    "    def process_source(self, source: str, year_month: str):\n",
    "        logger.info(f\"Processing {source} for {year_month}\")\n",
    "        \n",
    "        if self.storage.metadata_exists(year_month):\n",
    "            logger.warning(f\"Metadata already exists for {year_month}, skipping\")\n",
    "            return MetadataFile()\n",
    "\n",
    "        extractor = self.extractors[source]\n",
    "        raw_items = extractor.fetch_data(year_month)\n",
    "        \n",
    "        metadata = MetadataFile()\n",
    "        for item in raw_items:\n",
    "            content = item['content'].encode()\n",
    "            s3_path = f\"raw/{source}/{year_month.replace('-', '/')}/{item['id']}.{item['format']}\"\n",
    "            \n",
    "            entry = MetadataEntry.from_raw_file(\n",
    "                content=content,\n",
    "                source=source,\n",
    "                source_id=item['id'],\n",
    "                s3_path=s3_path\n",
    "            )\n",
    "            metadata.add_entry(entry)\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.storage.s3.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=s3_path,\n",
    "                Body=content\n",
    "            )\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def run(self):\n",
    "        current_month = datetime.utcnow().strftime('%Y-%m')\n",
    "        logger.info(f\"Starting extraction for {current_month}\")\n",
    "        \n",
    "        all_metadata = MetadataFile()\n",
    "        \n",
    "        for source in ['0_0', 'ctgov', '0_1']:\n",
    "            try:\n",
    "                source_metadata = self.process_source(source, current_month)\n",
    "                all_metadata.entries.extend(source_metadata.entries)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {source}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if all_metadata.entries:\n",
    "            metadata_key = self.storage.save_metadata(all_metadata, current_month)\n",
    "            self.trigger_transformation(metadata_key)\n",
    "\n",
    "        logger.info(\"Extraction completed\")\n",
    "\n",
    "    def trigger_transformation(self, metadata_key: str):\n",
    "        logger.info(\"Triggering transformation lambda\")\n",
    "        lambda_client = boto3.client('lambda')\n",
    "        lambda_client.invoke(\n",
    "            FunctionName=os.environ['TRANSFORMATION_LAMBDA_NAME'],\n",
    "            InvocationType='Event',\n",
    "            Payload=json.dumps({'metadata_key': metadata_key})\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    pipeline = ExtractionPipeline()\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from metadata.schema import MetadataFile\n",
    "from metadata.storage import MetadataStorage\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def transform_0_0(content: str) -> dict:\n",
    "    # Implementation for 0_0 transformation\n",
    "    return {...}\n",
    "\n",
    "def transform_ctgov(content: str) -> dict:\n",
    "    # Implementation for 0_2 transformation\n",
    "    return {...}\n",
    "\n",
    "def transform_0_1(content: str) -> dict:\n",
    "    # Implementation for 0_1 transformation\n",
    "    return {...}\n",
    "\n",
    "class TransformationPipeline:\n",
    "    def __init__(self):\n",
    "        self.bucket = os.environ['DATA_BUCKET']\n",
    "        self.storage = MetadataStorage(self.bucket)\n",
    "        \n",
    "    def process_entry(self, entry):\n",
    "        try:\n",
    "            # Download raw file\n",
    "            obj = self.storage.s3.get_object(Bucket=self.bucket, Key=entry.s3_raw_path)\n",
    "            content = obj['Body'].read().decode()\n",
    "            \n",
    "            # Transform based on source\n",
    "            if entry.source_system == '0_0':\n",
    "                transformed = transform_0_0(content)\n",
    "            elif entry.source_system == 'ctgov':\n",
    "                transformed = transform_ctgov(content)\n",
    "            elif entry.source_system == '0_1':\n",
    "                transformed = transform_0_1(content)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "            # Upload transformed file\n",
    "            transformed_key = f\"transformed/{entry.source_system}/{entry.s3_raw_path.split('/')[3]}/{entry.file_name.split('.')[0]}.csv\"\n",
    "            self.storage.s3.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=transformed_key,\n",
    "                Body=pd.DataFrame([transformed]).to_csv(index=False)\n",
    "            )\n",
    "            \n",
    "            # Update metadata\n",
    "            entry.transformation_status = \"COMPLETED\"\n",
    "            entry.transformed_s3_path = transformed_key\n",
    "            entry.last_updated = datetime.utcnow()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Transformation failed for {entry.file_id}: {str(e)}\")\n",
    "            entry.transformation_status = \"FAILED\"\n",
    "\n",
    "    def handle(self, metadata_key: str):\n",
    "        try:\n",
    "            metadata = self.storage.load_metadata(metadata_key)\n",
    "            \n",
    "            for entry in metadata.get_pending_transformations():\n",
    "                self.process_entry(entry)\n",
    "            \n",
    "            # Save updated metadata\n",
    "            new_key = metadata_key.replace('metadata/', 'metadata/processed/')\n",
    "            self.storage.save_metadata(metadata, new_key.split('/')[2], versioned=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Transformation pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    pipeline = TransformationPipeline()\n",
    "    metadata_key = event['metadata_key']\n",
    "    pipeline.handle(metadata_key)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': f\"Processed {metadata_key}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "boto3==1.34.25\n",
    "pandas==2.1.4\n",
    "requests==2.31.0\n",
    "tenacity==8.2.3\n",
    "python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0891351",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Env variables\n",
    "DATA_BUCKET=your-data-bucket\n",
    "TRANSFORMATION_LAMBDA_NAME=data-transformation-lambda\n",
    "0_0_API_KEY=your-key\n",
    "0_1_API_KEY=your-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS Setup:\n",
    "\n",
    "Create S3 bucket with versioning enabled\n",
    "\n",
    "Store API keys in AWS Secrets Manager\n",
    "\n",
    "Create ECS Fargate task with proper IAM permissions\n",
    "\n",
    "Configure Lambda with appropriate timeout/memory\n",
    "\n",
    "Monitoring:\n",
    "\n",
    "CloudWatch Alarms for:\n",
    "\n",
    "Failed transformations\n",
    "\n",
    "API request errors\n",
    "\n",
    "S3 upload failures\n",
    "\n",
    "X-Ray tracing for Lambda functions\n",
    "\n",
    "CloudTrail for API auditing\n",
    "\n",
    "This implementation provides:\n",
    "\n",
    "Centralized metadata management\n",
    "\n",
    "Production-grade error handling\n",
    "\n",
    "CloudWatch integration\n",
    "\n",
    "Modular architecture\n",
    "\n",
    "Easy schema evolution\n",
    "\n",
    "Source-specific processing\n",
    "\n",
    "Versioned metadata storage\n",
    "\n",
    "To use:\n",
    "\n",
    "Create the directory structure\n",
    "\n",
    "Copy files to appropriate locations\n",
    "\n",
    "Install dependencies\n",
    "\n",
    "Configure environment variables\n",
    "\n",
    "Deploy to AWS infrastructure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ab70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let me clarify where the **transformation requirement check** exists in the code and where we need improvements:\n",
    "\n",
    "### **Current Implementation Status** 🔍\n",
    "1. **Metadata Field**  \n",
    "   We have the `transformation_required` boolean field in the `MetadataEntry` class:\n",
    "   ```python\n",
    "   @dataclass\n",
    "   class MetadataEntry:\n",
    "       transformation_required: bool  # Exists in the schema\n",
    "   ```\n",
    "\n",
    "2. **Default Value**  \n",
    "   In the extraction logic, **all files** are marked as requiring transformation by default:\n",
    "   ```python\n",
    "   entry = MetadataEntry.from_raw_file(\n",
    "       # ...\n",
    "       transformation_required=True  # Hardcoded in from_raw_file()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Transformation Trigger**  \n",
    "   The Lambda processes entries where:\n",
    "   ```python\n",
    "   entry.transformation_required and entry.transformation_status == \"PENDING\"\n",
    "   ```\n",
    "\n",
    "### **What's Missing** ❌\n",
    "We need **conditional logic** to determine if transformation is actually required for each file.  \n",
    "This should be based on **business rules** like:\n",
    "- File type (e.g., XML vs. CSV)\n",
    "- Source system (e.g., 0_0 vs. 0_1)\n",
    "- Content patterns (e.g., presence of specific fields)\n",
    "\n",
    "---\n",
    "\n",
    "### **Enhanced Implementation** 🛠\n",
    "\n",
    "#### 1. **Add Transformation Check Logic** (`metadata/schema.py`)\n",
    "```python\n",
    "@dataclass\n",
    "class MetadataEntry:\n",
    "    # ... existing fields ...\n",
    "\n",
    "    @classmethod\n",
    "    def from_raw_file(cls, content: bytes, source: str, source_id: str, s3_path: str):\n",
    "        # Determine if transformation is needed\n",
    "        requires_transform = cls._check_transformation_required(\n",
    "            content=content,\n",
    "            source=source,\n",
    "            file_name=s3_path.split(\"/\")[-1]\n",
    "        )\n",
    "        \n",
    "        return cls(\n",
    "            # ...\n",
    "            transformation_required=requires_transform,  # Dynamic value\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _check_transformation_required(cls, content: bytes, source: str, file_name: str) -> bool:\n",
    "        \"\"\"Business rules for transformation requirement\"\"\"\n",
    "        # Example 1: File type check\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            return False  # Assume CSV files don't need transformation\n",
    "        \n",
    "        # Example 2: Source-specific logic\n",
    "        if source == \"0_0\":\n",
    "            return True  # Always transform 0_0 XML\n",
    "        \n",
    "        # Example 3: Content-based check\n",
    "        if b\"<clinical_study>\" in content:\n",
    "            return True  # Raw XML needs processing\n",
    "        \n",
    "        return False  # Default case\n",
    "```\n",
    "\n",
    "#### 2. **Update Extraction Logic** (`extraction/task.py`)\n",
    "```python\n",
    "def process_source(self, source: str, year_month: str):\n",
    "    # ...\n",
    "    for item in raw_items:\n",
    "        content = item['content'].encode()\n",
    "        s3_path = f\"raw/{source}/{year_month.replace('-', '/')}/{item['id']}.{item['format']}\"\n",
    "        \n",
    "        entry = MetadataEntry.from_raw_file(\n",
    "            content=content,\n",
    "            source=source,\n",
    "            source_id=item['id'],\n",
    "            s3_path=s3_path\n",
    "        )\n",
    "        # Now transformation_required is set based on actual checks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Improvements** 🚀\n",
    "1. **Dynamic Requirement Check**  \n",
    "   Files are now evaluated using:\n",
    "   - File extension\n",
    "   - Source system\n",
    "   - Content patterns\n",
    "\n",
    "2. **Business Rules Centralization**  \n",
    "   All transformation logic lives in `_check_transformation_required()`\n",
    "\n",
    "3. **Flexibility**  \n",
    "   Add/remove rules without changing pipeline flow:\n",
    "   ```python\n",
    "   def _check_transformation_required(...):\n",
    "       # New rule: Skip files smaller than 1KB\n",
    "       if len(content) < 1024:\n",
    "           return False\n",
    "       \n",
    "       # New rule: 0_1 JSON files need transformation\n",
    "       if source == \"0_1\" and file_name.endswith(\".json\"):\n",
    "           return True\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Architecture** 🏛\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Extract File] --> B{Check Transformation<br/>Required?}\n",
    "    B -->|Yes| C[Mark transformation_required=True]\n",
    "    B -->|No| D[Mark transformation_required=False]\n",
    "    C --> E[Trigger Lambda]\n",
    "    D --> F[Skip Transformation]\n",
    "```\n",
    "\n",
    "This ensures only files needing transformation are processed, optimizing costs and runtime. Would you like me to add specific business rules for your use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(self, entry):\n",
    "    try:\n",
    "        if not entry.transformation_required:\n",
    "            logger.info(f\"Skipping transformation for {entry.file_id} as it's not required.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting transformation for {entry.file_id} from source: {entry.source}\")\n",
    "\n",
    "        # Step 1: Download raw file content\n",
    "        obj = self.storage.s3.get_object(Bucket=self.bucket, Key=entry.s3_raw_path)\n",
    "        content = obj['Body'].read().decode()\n",
    "\n",
    "        # Step 2: Transform based on the source\n",
    "        if entry.source == \"0_1\":\n",
    "            transformed_content = self.0_1_transformer.transform(content)\n",
    "        elif entry.source == \"0_0\":\n",
    "            transformed_content = self.0_0_transformer.transform(content)\n",
    "        elif entry.source == \"clinical_trials\":\n",
    "            transformed_content = self.clinical_trials_transformer.transform(content)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown source '{entry.source}' for {entry.file_id}. Skipping transformation.\")\n",
    "            return\n",
    "\n",
    "        # Step 3: Upload transformed file to S3\n",
    "        transformed_key = f\"transformed/{entry.source}/{entry.file_id}.json\"\n",
    "        self.storage.s3.put_object(\n",
    "            Bucket=self.bucket,\n",
    "            Key=transformed_key,\n",
    "            Body=json.dumps(transformed_content)\n",
    "        )\n",
    "\n",
    "        # Step 4: Update metadata\n",
    "        entry.s3_transformed_path = transformed_key\n",
    "        entry.status = \"transformed\"\n",
    "        logger.info(f\"Transformation complete for {entry.file_id}, uploaded to {transformed_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error transforming {entry.file_id}: {str(e)}\")\n",
    "        entry.status = \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c126784",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"file_id\": \"abc123\",\n",
    "  \"source\": \"0_0\",\n",
    "  \"extraction_timestamp\": \"2025-05-22T14:30:00Z\",\n",
    "  \"extracted_by\": \"eventbridge-ecs-task\",\n",
    "  \"s3_raw_path\": \"raw/0_0/abc123.json\",\n",
    "  \"transformation_required\": true,\n",
    "  \"transformation_timestamp\": \"2025-05-22T14:45:00Z\",\n",
    "  \"transformed_by\": \"lambda-transformer-v1\",\n",
    "  \"s3_transformed_path\": \"transformed/0_0/abc123.json\",\n",
    "  \"status\": \"transformed\",  // could be \"extracted\", \"transformed\", \"error\"\n",
    "  \"error_message\": null,\n",
    "  \"file_size_bytes\": 34256,\n",
    "  \"content_type\": \"application/json\",\n",
    "  \"checksum_md5\": \"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"remarks\": \"Transformation completed successfully\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a3050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd94886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
