def lambda_handler(event, context):
    logger.info("Processing Event: VOX Sharepoint Extraction")
    
    try:
        source_s3 = boto3.client("s3")
        bucket_name = config.VOX_SHAREPOINT_53_BUCKET_NAME
        file_path = config.VOX_SHAREPOINT_FOLDER_NAME
        logger.info(f"Processing for {bucket_name} -- {file_path}")

        # Initialize variables
        success_copy_list, failed_copy_list = [], []
        current_batch_metadata = MetadataBatch(max_batch_size=config.MAX_UPLOAD_BATCH_SIZE)
        batches = [current_batch_metadata]
        time_threshold = get_current_time() - timedelta(hours=24)

        # Process S3 objects
        paginator = source_s3.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket_name, Prefix=file_path):
            if "Contents" not in page:
                continue

            for obj in page["Contents"]:
                # Skip directories and old files
                if obj["Key"].endswith("/") or obj["LastModified"].replace(tzinfo=None) <= time_threshold:
                    continue

                try:
                    # Process file copy
                    object_key = obj["Key"]
                    src_filename = os.path.basename(object_key)
                    file_extension = get_file_extension(object_key)
                    file_size_mb = format_file_size(obj["Size"])

                    copy_file_to_dest_s3(
                        src_bucket=bucket_name,
                        src_file_path=object_key,
                        dest_bucket=config.S3_BUCKET_NAME,
                        dest_folder=f"{config.S3_DATA_FILES_BASE_PATH}",
                    )

                    # Create metadata entry
                    parts = object_key.split('/')
                    datatype = parts[-2]
                    data_source_details = get_data_source_details("ESMO" if "ESMO" in datatype else "NCCN")

                    entry = MetadataEntry(
                        file_name=src_filename,
                        source=data_source_details["data_source_name"],
                        source_url=config.VOX_SHAREPOINT_URL,
                        extraction_timestamp=get_formatted_time(drop_precision=3),
                        s3_path=f"{config.S3_DATA_FILES_BASE_PATH}/{object_key}",
                        transformation_required=False,
                        file_size_bytes=file_size_mb,
                        content_type=file_extension,
                        document_chunking_strategy=config.CHUNKING_STRATEGY,
                        engine=config.EMBEDDING_ENGINE,
                        data_product_alias=data_source_details["data_product_name"],
                        creation_date_at_source=get_formatted_time(drop_precision=3),
                    )

                    # Handle batching
                    if current_batch_metadata.size >= config.MAX_UPLOAD_BATCH_SIZE:
                        current_batch_metadata = MetadataBatch(max_batch_size=config.MAX_UPLOAD_BATCH_SIZE)
                        batches.append(current_batch_metadata)

                    current_batch_metadata.add(entry)
                    success_copy_list.append(obj)

                except Exception as e:
                    logger.error(f"Error copying file {object_key}: {e}")
                    failed_copy_list.append(object_key)

        # Log results
        if failed_copy_list:
            logger.info(f"Failed to copy {len(failed_copy_list)} files.\nFailed filenames: {failed_copy_list}")

        if success_copy_list:
            for batch in batches:
                metadata_file_name = generate_thread_specific_filename(prefix="Vox SharePoint")
                batch_df = batch.to_dataframe()
                awsHelper.upload_dataframe_to_s3(
                    df=batch_df,
                    file_name=metadata_file_name,
                    file_format="csv",
                    file_location=config.S3_METADATA_DIRECTORY_PATH,
                    bucket_name=config.S3_BUCKET_NAME,
                )

        return {
            "statusCode": status.HTTP_200_OK,
            "body": f"Process complete for {bucket_name} {file_path}",
        }

    except Exception as err:
        logger.error(f"Error while processing event data: {err}")
        raise err
