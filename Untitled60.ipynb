## Helper functions
import hashlib

from typing import List, Optional

import httpx

from cachetools import TTLCache from fastapi import HTTPException

from layers.common.config import ( CACHE TTL FILTERED_PROMPTS, CACHE TTL PROMPTS, PROMPT LIBRARY_URL, )

from layers.common.custom_logger import logger

prompt_cache TTLCache(maxsize-1000, ttl-CACHE_TTL_PROMPTS) filtered_cache TTLCache(maxsize-500, ttl-CACHE_TTL_FILTERED_PROMPTS)

def preprocess_prompts(prompts: List[dict]) -> List[dict]:

**"Pre-compute expensive operations for faster filtering"""

for prompt in prompts:

#Pre-compute lowercase tag sets

prompt["_tag_set"] [t.lower() for t in prompt.get("tags", [])) #Pre-compute lowercase prompt text prompt["_prompt_lower"]- prompt.get("prompt", "").lower() #Pre-extract modified at for faster sorting prompt["_modified_at"] "modified_at" prompt.get("metadata_info", ()).get(

)or prompt.get("modified_at", "")

return prompts

def get_filter_cache_key(

tags: Optional [List[str]], keyword: Optional [str], sort_by: str

) -> str:

**"Generate cache key for filtered results""" tags str

keyword_str keyword or ""

,".join(sorted (tags)) if tags else

key_dataf"[tags_str}_{keyword_str}_{sort_by}"

return hashlib.md5(key_data.encode()).hexdigest()

def filter_and_sort_prompts_optimized(

prompts: List[dict],

tags: Optional [List[str]] None,

keyword: Optional [str] - None,

sort_by: str "modified_at",

) -> List[dict]:

"""Optimized filter and sort using pre-computed data"""

if not prompts:

return []

filtered prompts

#Tag filtering with pre-computed sets

if tags:

tags_lower (tag.strip().lower() for tag in tags if tag.strip()}

if tags lower:

I

filtered [p for p in filtered if tags_lower & p.get("_tag_set", set())]

if not filtered: Early exit

return []

#Keyword filtering with pre-computed lowercase text

if keyword:

keyword_lower keyword.lower()

scored prompts = []

prompt text p.get("_prompt_lower", "")

for p in filtered:

if keyword lower in prompt_text:

#Calculate score in single pass

count prompt_text.count(keyword_lower)

starts with 18 if prompt text.startswith(keyword_lower) else 0


modified p.get("_modified_at", "")

scored_prompts.append((count + starts_with, modified, p))

if not scored_prompts:

return []

#Return sorted by score then date

return [

for

pin sorted(

scored_prompts, key-lambda x: (-x[8], x[1])

# Negative for descending

#Default sorting with pre-computed modified_at

If sort by

else:

"modified at":

filtered.sort(key-lambda x: x.get("_modified_at",

""), reverse-True)

filtered.sort(key-lambda x: x.get(sort_by, ""), reverse-True)

return filtered

async def fetch_prompts(

sso token: str, auth token: str, force_refresh: bool - False

List[dict]:

"""Fetch prompts from the Vox library with TTL caching"

cache key "all_prompts"

Check cache (TTLCache handles TTL automatically)

If not force_refresh and cache key in prompt_cache:

logger.debug("Returning cached prompts") return prompt_cache[cache_key]

logger.info("Fetching fresh prompts from library")

headers = {

}

"x-auth": f"Bearer (sso_token}", "Authorization": f"Bearer (auth_token}", "User-Agent": "PharmaResearch-ChatBot/1.0",

try:

async with httpx.AsyncClient(

timeout-httpx.Timeout(30.0, read-68.0), limits-httpx.Limits(max_connections-18), ) as client: response await client.get(PROMPT_LIBRARY_URL, headers-headers) response.raise_for_status()

data response.json()

If not data.get("success", False):

error_msg- data.get("message", "Invalid response from prompt library") I

logger.error("Prompt library error: (error_msg)")

Return cached data if available during errors

If cache key in prompt_cache:

logger.warning("Returning stale cached data due to API error")

return prompt cache[cache_key]

raise HTTPException(status_code-400, detail-error_msg)

prompts data.get("details", {}).get("prompts", []) logger.info(f"Fetched (len(prompts)} prompts")

#Pre-process prompts for faster filtering processed_prompts preprocess_prompts(prompts)

#TTLCache handles expiry automatically prompt_cache[cache_key] processed_prompts

#clear filtered cache when raw prompts are updated filtered_cache.clear()

return processed_prompts

except (httpx.TimeoutException, httpx.RequestError) as e: logger.error(f"Request failed: (str(e)]", exc_info-True)

#Graceful degradation return cached data if available

if cache key in prompt_cache: logger.warning("Returning cached data due to network error") return prompt_cache[cache_key]

error_code 504 if isinstance(e, httpx.TimeoutException) else 503

error msg( "Prompt library timeout"

if isinstance(e, httpx.TimeoutException)

else "Prompt library service unavailable"

raise HTTPException(status_code-error_code, detail-error_msg)

except Exception as e:

I

logger.error(f"Failed to fetch prompts: (str(e)}", exc_info-True)

Return cached data if available

If cache key in prompt_cache:

logger.warning("Returning cached data due to unexpected error") return prompt_cache[cache_key]

raise HTTPException(status_code-500, detail-str(e))

async def get_filtered_prompts(

sso_token: str,

auth_token: str,

tags: Optional [List[str]] - None,

keyword: Optional[str] - None,

sort by: str "modified_at",

)-> List[dict]:

"""Get filtered and sorted prompts with multi-level caching"""

Check filtered cache first

filter key get_filter_cache_key(tags, keyword, sort_by) logger.debug("Returning cached filtered results")

if filter key in filtered_cache:

return filtered_cache[filter_key]

#Get raw prompts (from cache or API)

raw_prompts await fetch_prompts(sso_token, auth_token)

Apply optimized filtering

filtered_results filter_and_sort_prompts_optimized( raw prompts, tags, keyword, sort_by )

I

Cache the filtered results

filtered_cache[filter_key] filtered_results

logger.debug(f"Cached filtered results with key: (filter_key)")

return filtered_results

##FastAPI

from typing import List, Optional

from cachetools import TTLCache

from fastapi import APIRouter, HTTPException, Query, Request, status

from backend.services.prompts_utils import get_filtered_prompts

from layers.common.config import ( CACHE_TTL_MULE_TOKEN, PINGFEDERATE_URL, PROMPT_DISPLAY_LIMIT, VOX_CLIENT_ID, VOX_CLIENT_SECRET,

) from layers.common.custom_logger import logger

from layers.database.models.prompts import PromptSuggestion, SuggestionsResponse from layers.vox_components_utility.token_generator import generate_api_token

router APIRouter()

auth_token_cache TTLCache (maxsize-10, ttl-CACHE_TTL_MULE_TOKEN)

async def get_cached_auth_token():

"""Get cached auth token to avoid repeated calls""" cache key F"[VOX_CLIENT_ID}_{VOX_CLIENT_SECRET}"

if cache key in auth_token_cache: logger.debug("Returning cached auth token") return auth_token_cache[cache_key]

logger.info("Generating new auth token") token generate_api_token (PINGFEDERATE_URL, VOX_CLIENT_ID, VOX_CLIENT_SECRET) auth_token_cache[cache_key] token return token


@router.get(

"/list",

)

status_code-status.HTTP_200_OK,

tags-["Prompts"],

description="Get top N prompts according to data product and key word",

response_model-Suggestions Response,

async def get prompts(

request: Request,

keyword: Optional [str] Query(

), None, min_length-1, description-"Search term to filter prompts"

data_products: Optional [List [str]] - Query(

), None, description-"List of data products to filter by", alias="data_product"

try:

logger.info(

"Fetching prompts with keyword='%s' and data_products-%s",

keyword,

data_products,

authorization request.headers.get("Authorization")

sso token authorization[7:]

Use cached auth token instead of generating every time auth token await get_cached_auth_token()

#fetching and filtering Prompts

filtered_prompts await get_filtered_prompts(

)

sso_token=sso_token,

auth_token-auth_token,

tags=data_products,

keyword-keyword,

sort_by="modified_at",

#Early return if no prompts

if not filtered_prompts:

return SuggestionsResponse( ) suggestions-[], data_products-data_products, keyword-keyword, count-0,

logger.info(f"Filtered (len(filtered_prompts)} prompts")

#Limit results early to avoid unnecessary processing

limit = min(int (PROMPT_DISPLAY_LIMIT), len (filtered_prompts))

limited_prompts filtered_prompts[:limit]

#Format response with consistent data access

suggestions- [

PromptSuggestion(

id-p.get("id", ""),

prompt-p.get("prompt", ""),

I

data_product-p.get("tags", []).

last_modified-p.get("metadata_info", {}).get("modified_at")

or p.get("modified_at", ""),

for p in limited_prompts

return SuggestionsResponse(

suggestions-suggestions,

data products-data_products,

keyword-keyword,

count-len (filtered_prompts), Total count before limiting
)

except HTTPException:

raise

except Exception as e:

logger.exception("Unexpected error occurred:") raise HTTPException(status_code-500, detall-"Internal server error") 1

@router.get("/health")

async def health_check(): return {"status": "healthy"}
