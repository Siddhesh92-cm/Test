{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8bef80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, HTTPException, Query, status\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONResponse\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsonable_encoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException, Query, status\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Prompt Library API\",\n",
    "    description=\"API for retrieving top prompts for data products\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Configuration from environment variables\n",
    "class Config:\n",
    "    PROMPT_LIBRARY_API_URL = os.getenv(\"PROMPT_LIBRARY_API_URL\", \"https://prompt-library.example.com\")\n",
    "    SSO_TOKEN = os.getenv(\"SSO_TOKEN\")\n",
    "    MULE_TOKEN = os.getenv(\"MULE_TOKEN\")\n",
    "    REQUEST_TIMEOUT = int(os.getenv(\"REQUEST_TIMEOUT\", \"10\"))  # seconds\n",
    "\n",
    "    @staticmethod\n",
    "    def get_auth_headers():\n",
    "        return {\n",
    "            'x-auth': Config.SSO_TOKEN,\n",
    "            'Authorization': f'Bearer {Config.MULE_TOKEN}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "# Validate required environment variables\n",
    "if not Config.SSO_TOKEN or not Config.MULE_TOKEN:\n",
    "    raise RuntimeError(\"SSO_TOKEN and MULE_TOKEN environment variables must be set\")\n",
    "\n",
    "# Models\n",
    "class PromptMetadata(BaseModel):\n",
    "    model: Optional[str]\n",
    "    temperature: Optional[float]\n",
    "    max_word_count: Optional[int]\n",
    "    section: Optional[str]\n",
    "    data_product: Optional[str] = Field(alias=\"dataProduct\")\n",
    "    tags: Optional[List[str]]\n",
    "    is_visible: Optional[bool] = Field(alias=\"isVisible\")\n",
    "    created_at: Optional[datetime] = Field(alias=\"createdAt\")\n",
    "    created_by: Optional[str] = Field(alias=\"createdBy\")\n",
    "    modified_at: Optional[datetime] = Field(alias=\"modifiedAt\")\n",
    "    modified_by: Optional[str] = Field(alias=\"modifiedBy\")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    system_prompt: Optional[str] = Field(alias=\"systemPrompt\")\n",
    "    prompt: str\n",
    "    metadata: PromptMetadata\n",
    "\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "# Error Handling\n",
    "class ExternalAPIError(Exception):\n",
    "    pass\n",
    "\n",
    "@app.exception_handler(ExternalAPIError)\n",
    "async def external_api_error_handler(request, exc):\n",
    "    logger.error(f\"External API error: {str(exc)}\")\n",
    "    return JSONResponse(\n",
    "        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "        content={\"message\": \"Error communicating with prompt library service\"}\n",
    "    )\n",
    "\n",
    "# Utility Functions\n",
    "def fetch_prompts_from_library() -> List[Dict]:\n",
    "    \"\"\"Fetch all prompts from the external API\"\"\"\n",
    "    url = f\"{Config.PROMPT_LIBRARY_API_URL}/prompts\"  # Adjust endpoint as needed\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=Config.get_auth_headers(),\n",
    "            timeout=Config.REQUEST_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to fetch prompts: {str(e)}\")\n",
    "        raise ExternalAPIError(str(e))\n",
    "\n",
    "# Core Business Logic\n",
    "def filter_and_rank_prompts(\n",
    "    prompts: List[Prompt],\n",
    "    data_product: Optional[str] = None,\n",
    "    top_n: int = 5\n",
    ") -> List[Prompt]:\n",
    "    \"\"\"\n",
    "    Filter prompts by data product (if specified) and return top N by creation date\n",
    "    \"\"\"\n",
    "    filtered = prompts\n",
    "    \n",
    "    if data_product:\n",
    "        filtered = [\n",
    "            p for p in filtered \n",
    "            if p.metadata.data_product and \n",
    "            p.metadata.data_product.lower() == data_product.lower()\n",
    "        ]\n",
    "    \n",
    "    # Sort by creation date (newest first)\n",
    "    return sorted(\n",
    "        filtered,\n",
    "        key=lambda x: (-x.metadata.created_at.timestamp() if x.metadata.created_at else 0)\n",
    "    )[:top_n]\n",
    "\n",
    "# API Endpoint\n",
    "@app.get(\n",
    "    \"/prompts/top\",\n",
    "    response_model=List[Prompt],\n",
    "    summary=\"Get top prompts\",\n",
    "    description=\"Retrieve the top N prompts for a specific data product or across all products\",\n",
    "    responses={\n",
    "        200: {\"description\": \"Successful operation\"},\n",
    "        400: {\"description\": \"Invalid parameters\"},\n",
    "        503: {\"description\": \"Prompt library service unavailable\"}\n",
    "    }\n",
    ")\n",
    "async def get_top_prompts(\n",
    "    top_n: int = Query(..., gt=0, le=100, description=\"Number of top prompts to return\"),\n",
    "    data_product: Optional[str] = Query(\n",
    "        None,\n",
    "        min_length=2,\n",
    "        max_length=50,\n",
    "        description=\"Filter prompts by data product\"\n",
    "    )\n",
    "):\n",
    "    try:\n",
    "        # Fetch prompts from external API\n",
    "        prompts_data = fetch_prompts_from_library()\n",
    "        \n",
    "        # Parse and validate\n",
    "        try:\n",
    "            prompts = [Prompt.parse_obj(p) for p in prompts_data]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing prompt data: {str(e)}\")\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Error processing prompt data\"\n",
    "            )\n",
    "        \n",
    "        # Filter and rank\n",
    "        top_prompts = filter_and_rank_prompts(prompts, data_product, top_n)\n",
    "        return jsonable_encoder(top_prompts)\n",
    "    \n",
    "    except ExternalAPIError:\n",
    "        raise  # Handled by custom exception handler\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=\"An unexpected error occurred\"\n",
    "        )\n",
    "\n",
    "# Health Check Endpoint\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd2e10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, HTTPException, Query, Depends, status\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONResponse\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsonable_encoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException, Query, Depends, status\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional, List, Dict, Any\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Prompt Library API\",\n",
    "    description=\"API for retrieving top prompts for data products\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    PROMPT_LIBRARY_API_URL = \"https://URL\"\n",
    "    AUTH_HEADERS = {\n",
    "        'x-auth': 'token',\n",
    "        'Authorization': 'Bearer token',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    REQUEST_TIMEOUT = 10  # seconds\n",
    "    CACHE_TTL = 300  # 5 minutes\n",
    "\n",
    "# Models\n",
    "class PromptMetadata(BaseModel):\n",
    "    model: Optional[str]\n",
    "    temperature: Optional[float]\n",
    "    max_word_count: Optional[int]\n",
    "    section: Optional[str]\n",
    "    data_product: Optional[str] = Field(alias=\"dataProduct\")  # Handle camelCase in response\n",
    "    tags: Optional[List[str]]\n",
    "    is_visible: Optional[bool] = Field(alias=\"isVisible\")\n",
    "    created_at: Optional[datetime] = Field(alias=\"createdAt\")\n",
    "    created_by: Optional[str] = Field(alias=\"createdBy\")\n",
    "    modified_at: Optional[datetime] = Field(alias=\"modifiedAt\")\n",
    "    modified_by: Optional[str] = Field(alias=\"modifiedBy\")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    system_prompt: Optional[str] = Field(alias=\"systemPrompt\")\n",
    "    prompt: str\n",
    "    metadata: PromptMetadata\n",
    "\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "# Error Handling\n",
    "class ExternalAPIError(Exception):\n",
    "    pass\n",
    "\n",
    "@app.exception_handler(ExternalAPIError)\n",
    "async def external_api_error_handler(request, exc):\n",
    "    logger.error(f\"External API error: {str(exc)}\")\n",
    "    return JSONResponse(\n",
    "        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "        content={\"message\": \"Error communicating with prompt library service\"}\n",
    "    )\n",
    "\n",
    "# Utility Functions\n",
    "def make_external_api_call(url: str, method: str = \"GET\", payload: Optional[Dict] = None):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.request(\n",
    "            method,\n",
    "            url,\n",
    "            headers=Config.AUTH_HEADERS,\n",
    "            json=payload,\n",
    "            timeout=Config.REQUEST_TIMEOUT\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        logger.info(f\"External API call to {url} took {duration:.2f} seconds\")\n",
    "\n",
    "        if response.status_code >= 400:\n",
    "            logger.error(f\"External API error: {response.status_code} - {response.text}\")\n",
    "            raise ExternalAPIError(f\"Status code: {response.status_code}\")\n",
    "\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout when calling external API: {url}\")\n",
    "        raise ExternalAPIError(\"Request timeout\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request error when calling external API: {str(e)}\")\n",
    "        raise ExternalAPIError(str(e))\n",
    "\n",
    "@lru_cache(maxsize=128, ttl=Config.CACHE_TTL)\n",
    "def get_all_prompts_cached():\n",
    "    \"\"\"Cache the prompts to reduce external API calls\"\"\"\n",
    "    url = f\"{Config.PROMPT_LIBRARY_API_URL}/prompts\"  # Assuming there's an endpoint to get all prompts\n",
    "    return make_external_api_call(url)\n",
    "\n",
    "# Core Business Logic\n",
    "def filter_and_rank_prompts(\n",
    "    prompts: List[Prompt],\n",
    "    data_product: Optional[str] = None,\n",
    "    top_n: int = 5\n",
    ") -> List[Prompt]:\n",
    "    \"\"\"\n",
    "    Filter prompts by data product (if specified) and return top N by creation date (newest first)\n",
    "    \"\"\"\n",
    "    filtered_prompts = prompts\n",
    "    \n",
    "    # Filter by data product if specified\n",
    "    if data_product:\n",
    "        filtered_prompts = [\n",
    "            p for p in filtered_prompts \n",
    "            if p.metadata.data_product and p.metadata.data_product.lower() == data_product.lower()\n",
    "        ]\n",
    "    \n",
    "    # Sort by creation date (newest first)\n",
    "    sorted_prompts = sorted(\n",
    "        filtered_prompts,\n",
    "        key=lambda x: (-x.metadata.created_at.timestamp() if x.metadata.created_at else 0)\n",
    "    )\n",
    "    \n",
    "    return sorted_prompts[:top_n]\n",
    "\n",
    "# API Endpoint\n",
    "@app.get(\n",
    "    \"/prompts/top\",\n",
    "    response_model=List[Prompt],\n",
    "    summary=\"Get top prompts\",\n",
    "    description=\"Retrieve the top N prompts for a specific data product or across all products\",\n",
    "    responses={\n",
    "        200: {\"description\": \"Successful operation\"},\n",
    "        400: {\"description\": \"Invalid parameters\"},\n",
    "        503: {\"description\": \"Prompt library service unavailable\"}\n",
    "    }\n",
    ")\n",
    "async def get_top_prompts(\n",
    "    top_n: int = Query(..., gt=0, le=100, description=\"Number of top prompts to return\"),\n",
    "    data_product: Optional[str] = Query(\n",
    "        None,\n",
    "        min_length=2,\n",
    "        max_length=50,\n",
    "        description=\"Filter prompts by data product\"\n",
    "    )\n",
    "):\n",
    "    try:\n",
    "        # Get all prompts (using cache if available)\n",
    "        all_prompts_data = get_all_prompts_cached()\n",
    "        \n",
    "        # Parse and validate the prompts\n",
    "        try:\n",
    "            all_prompts = [Prompt.parse_obj(prompt) for prompt in all_prompts_data]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing prompt data: {str(e)}\")\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Error processing prompt data\"\n",
    "            )\n",
    "        \n",
    "        # Filter and rank prompts\n",
    "        top_prompts = filter_and_rank_prompts(all_prompts, data_product, top_n)\n",
    "        \n",
    "        return jsonable_encoder(top_prompts)\n",
    "    \n",
    "    except ExternalAPIError as e:\n",
    "        raise  # This will be handled by our custom exception handler\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=\"An unexpected error occurred\"\n",
    "        )\n",
    "\n",
    "# Health Check Endpoint\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Startup Event\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    logger.info(\"Starting up Prompt Library API service\")\n",
    "    # Warm up the cache\n",
    "    try:\n",
    "        get_all_prompts_cached()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to warm up cache on startup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37254419",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, Header, HTTPException, Query\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, List\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Header, HTTPException, Query\n",
    "from typing import Optional, List\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "def get_prompts_from_library(sso_token: str, mule_token: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Fetches prompts from the external Prompt Library API.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {sso_token}\",\n",
    "        \"X-MULE-TOKEN\": mule_token\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://prompt-library.example.com/prompts\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching prompts from external API: {e}\")\n",
    "        raise HTTPException(status_code=502, detail=\"Failed to fetch data from Prompt Library\")\n",
    "\n",
    "\n",
    "@app.get(\"/top-prompts\")\n",
    "def get_top_prompts(\n",
    "    top_n: int = Query(..., gt=0, le=100, description=\"Number of top prompts to return (1-100)\"),\n",
    "    data_product: Optional[str] = Query(None, description=\"Optional data product to filter prompts by\"),\n",
    "    sso_token: str = Header(..., alias=\"Authorization\"),\n",
    "    mule_token: str = Header(..., alias=\"X-MULE-TOKEN\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the top N prompts for a given data product.\n",
    "    If data_product is not provided, returns overall top N prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts = get_prompts_from_library(sso_token, mule_token)\n",
    "\n",
    "    # Filter by data_product if provided\n",
    "    if data_product:\n",
    "        prompts = [p for p in prompts if p.get(\"metadata\", {}).get(\"data_product\") == data_product]\n",
    "\n",
    "    if not prompts:\n",
    "        return {\"prompts\": []}\n",
    "\n",
    "    # Example: Sort by 'rank' field in metadata if it exists\n",
    "    sorted_prompts = sorted(\n",
    "        prompts,\n",
    "        key=lambda p: p.get(\"metadata\", {}).get(\"rank\", 0),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return {\"prompts\": sorted_prompts[:top_n]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df885d",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2eac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Depends, HTTPException, Request, Query\n",
    "from fastapi.security import HTTPBearer\n",
    "from typing import Optional, List\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from cachetools import TTLCache\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Prompt Library API\",\n",
    "    description=\"API to fetch top N prompts from the prompt library\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Constants\n",
    "PROMPT_LIBRARY_URL = \"https:url\"\n",
    "CACHE_TTL_SECONDS = 300  # 5 minutes cache\n",
    "MAX_PROMPTS_LIMIT = 100\n",
    "\n",
    "# Cache setup\n",
    "prompt_cache = TTLCache(maxsize=100, ttl=CACHE_TTL_SECONDS)\n",
    "\n",
    "# Security\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Models\n",
    "class MetadataInfo(BaseModel):\n",
    "    navigator_domain: Optional[List[str]] = Field(None, alias=\"navigator_domain\")\n",
    "    data_product: Optional[List[str]] = Field(None, alias=\"data_product\")\n",
    "    status: str\n",
    "    is_visible: bool = Field(..., alias=\"is visible\")\n",
    "    creator_tenant_id: str = Field(..., alias=\"creator tenant Id\")\n",
    "    modified_by: str = Field(..., alias=\"modified by\")\n",
    "    modified_at: datetime = Field(..., alias=\"modified_at\")\n",
    "    prompt: str\n",
    "    version: int\n",
    "    tags: Optional[str] = None\n",
    "    is_private: bool = Field(..., alias=\"is private\")\n",
    "    created_by: str = Field(..., alias=\"created by\")\n",
    "    created_at: datetime = Field(..., alias=\"created at\")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    name: str\n",
    "    system_prompt: str = Field(\"\", alias=\"system prompt\")\n",
    "    id: str\n",
    "    metadata_info: MetadataInfo = Field(..., alias=\"metadata_info\")\n",
    "\n",
    "class PromptLibraryResponse(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "class PromptResponse(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    prompt_text: str\n",
    "    data_product: Optional[List[str]]\n",
    "    version: int\n",
    "    created_at: datetime\n",
    "    modified_at: datetime\n",
    "\n",
    "# Utility functions\n",
    "def generate_token() -> str:\n",
    "    \"\"\"\n",
    "    Generate authorization token for the prompt library API\n",
    "    \"\"\"\n",
    "    return \"your_generated_token_here\"\n",
    "\n",
    "def extract_sso_token(request: Request) -> str:\n",
    "    \"\"\"\n",
    "    Extract SSO token from the incoming request headers\n",
    "    \"\"\"\n",
    "    auth_header = request.headers.get(\"x-auth\")\n",
    "    if not auth_header:\n",
    "        raise HTTPException(\n",
    "            status_code=401,\n",
    "            detail=\"Missing x-auth header\"\n",
    "        )\n",
    "    \n",
    "    # Handle both \"Bearer token\" and just \"token\" formats\n",
    "    if auth_header.startswith(\"Bearer \"):\n",
    "        return auth_header[7:]\n",
    "    return auth_header\n",
    "\n",
    "def handle_errors(func):\n",
    "    @wraps(func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request to prompt library failed: {str(e)}\")\n",
    "            raise HTTPException(\n",
    "                status_code=502,\n",
    "                detail=\"Failed to connect to prompt library service\"\n",
    "            )\n",
    "        except HTTPException:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=\"Internal server error\"\n",
    "            )\n",
    "    return wrapper\n",
    "\n",
    "# Service functions\n",
    "@handle_errors\n",
    "async def fetch_all_prompts(sso_token: str) -> List[Prompt]:\n",
    "    \"\"\"\n",
    "    Fetch all prompts from the prompt library with caching\n",
    "    \"\"\"\n",
    "    cache_key = \"all_prompts\"\n",
    "    if cache_key in prompt_cache:\n",
    "        logger.info(\"Returning prompts from cache\")\n",
    "        return prompt_cache[cache_key]\n",
    "    \n",
    "    headers = {\n",
    "        'x-auth': f'Bearer {sso_token}',\n",
    "        'Authorization': f'Bearer {generate_token()}'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(PROMPT_LIBRARY_URL, headers=headers, timeout=10)\n",
    "    if not response.ok:\n",
    "        logger.error(f\"Prompt library returned error: {response.status_code} - {response.text}\")\n",
    "        raise HTTPException(\n",
    "            status_code=response.status_code,\n",
    "            detail=\"Failed to fetch prompts from library\"\n",
    "        )\n",
    "    \n",
    "    data = response.json()\n",
    "    if not data.get(\"success\"):\n",
    "        logger.error(f\"Prompt library returned unsuccessful response: {data.get('message')}\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=data.get(\"message\", \"Prompt library request failed\")\n",
    "        )\n",
    "    \n",
    "    prompts = []\n",
    "    for prompt_data in data.get(\"details\", {}).get(\"prompts\", []):\n",
    "        try:\n",
    "            prompt = Prompt(**prompt_data)\n",
    "            if prompt.metadata_info.is_visible and prompt.metadata_info.status == \"APPROVED\":\n",
    "                prompts.append(prompt)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping invalid prompt data: {str(e)}\")\n",
    "    \n",
    "    prompt_cache[cache_key] = prompts\n",
    "    return prompts\n",
    "\n",
    "def filter_prompts_by_data_product(prompts: List[Prompt], data_product: Optional[str]) -> List[Prompt]:\n",
    "    \"\"\"\n",
    "    Filter prompts by data product if specified\n",
    "    \"\"\"\n",
    "    if not data_product:\n",
    "        return prompts\n",
    "    \n",
    "    filtered = []\n",
    "    for prompt in prompts:\n",
    "        if (prompt.metadata_info.data_product and \n",
    "            data_product in prompt.metadata_info.data_product):\n",
    "            filtered.append(prompt)\n",
    "    return filtered\n",
    "\n",
    "def sort_and_limit_prompts(prompts: List[Prompt], limit: int) -> List[Prompt]:\n",
    "    \"\"\"\n",
    "    Sort prompts by modification date (newest first) and limit results\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        prompts,\n",
    "        key=lambda x: x.metadata_info.modified_at,\n",
    "        reverse=True\n",
    "    )[:limit]\n",
    "\n",
    "def transform_prompt_response(prompts: List[Prompt]) -> List[PromptResponse]:\n",
    "    \"\"\"\n",
    "    Transform prompts to the response model\n",
    "    \"\"\"\n",
    "    return [\n",
    "        PromptResponse(\n",
    "            id=prompt.id,\n",
    "            name=prompt.name,\n",
    "            prompt_text=prompt.metadata_info.prompt,\n",
    "            data_product=prompt.metadata_info.data_product,\n",
    "            version=prompt.metadata_info.version,\n",
    "            created_at=prompt.metadata_info.created_at,\n",
    "            modified_at=prompt.metadata_info.modified_at\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "# API Endpoint\n",
    "@app.get(\n",
    "    \"/prompts/\",\n",
    "    response_model=List[PromptResponse],\n",
    "    summary=\"Get top N prompts\",\n",
    "    description=\"Fetch top N prompts from the prompt library, optionally filtered by data product\",\n",
    "    tags=[\"Prompts\"]\n",
    ")\n",
    "async def get_top_prompts(\n",
    "    request: Request,\n",
    "    data_product: Optional[str] = Query(\n",
    "        None,\n",
    "        description=\"Filter prompts by data product\",\n",
    "        examples=[\"data product 1\", \"data product 2\"]\n",
    "    ),\n",
    "    limit: int = Query(\n",
    "        10,\n",
    "        description=\"Number of prompts to return (1-100)\",\n",
    "        gt=0,\n",
    "        le=MAX_PROMPTS_LIMIT,\n",
    "        example=5\n",
    "    )\n",
    "):\n",
    "    \"\"\"\n",
    "    Get top N prompts from the prompt library.\n",
    "    \n",
    "    - **data_product**: Optional filter for specific data product\n",
    "    - **limit**: Number of prompts to return (default: 10, max: 100)\n",
    "    \n",
    "    Returns:\n",
    "    - List of prompts sorted by modification date (newest first)\n",
    "    - Only APPROVED and visible prompts are included\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract SSO token from request\n",
    "        sso_token = extract_sso_token(request)\n",
    "        \n",
    "        # Fetch all prompts (cached)\n",
    "        all_prompts = await fetch_all_prompts(sso_token)\n",
    "        \n",
    "        # Filter by data product if specified\n",
    "        filtered_prompts = filter_prompts_by_data_product(all_prompts, data_product)\n",
    "        \n",
    "        if not filtered_prompts:\n",
    "            logger.info(f\"No prompts found for data_product={data_product}\")\n",
    "            return []\n",
    "        \n",
    "        # Sort and limit results\n",
    "        top_prompts = sort_and_limit_prompts(filtered_prompts, limit)\n",
    "        \n",
    "        # Transform to response model\n",
    "        return transform_prompt_response(top_prompts)\n",
    "        \n",
    "    except HTTPException as he:\n",
    "        logger.error(f\"HTTP error in get_top_prompts: {he.detail}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in get_top_prompts: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"Internal server error\"\n",
    "        )\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Ready endpoint for Kubernetes\n",
    "@app.get(\"/ready\", include_in_schema=False)\n",
    "async def ready_check():\n",
    "    return {\"status\": \"ready\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like google\n",
    "\n",
    "from fastapi import FastAPI, Request, Query, HTTPException\n",
    "from typing import Optional, List\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "from cachetools import TTLCache\n",
    "import logging\n",
    "from functools import wraps\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "# ... (keep all the previous imports and setup code)\n",
    "\n",
    "# Add new models\n",
    "class SuggestionRequest(BaseModel):\n",
    "    partial_prompt: str\n",
    "    data_product: Optional[str] = None\n",
    "    limit: Optional[int] = 5\n",
    "\n",
    "class SuggestionResponse(BaseModel):\n",
    "    suggestions: List[str]\n",
    "\n",
    "# Add new service functions\n",
    "def find_similar_prompts(prompts: List[Prompt], partial_prompt: str, limit: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find prompts similar to the partial input using fuzzy matching\n",
    "    \"\"\"\n",
    "    if not partial_prompt:\n",
    "        return []\n",
    "    \n",
    "    # Get all prompt texts\n",
    "    prompt_texts = [p.metadata_info.prompt.lower() for p in prompts]\n",
    "    \n",
    "    # Find close matches\n",
    "    matches = get_close_matches(\n",
    "        partial_prompt.lower(),\n",
    "        prompt_texts,\n",
    "        n=limit,\n",
    "        cutoff=0.3  # Similarity threshold\n",
    "    )\n",
    "    \n",
    "    # Return the original prompt text (not lowercased)\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        for prompt in prompts:\n",
    "            if prompt.metadata_info.prompt.lower() == match:\n",
    "                results.append(prompt.metadata_info.prompt)\n",
    "                break\n",
    "                \n",
    "    return results\n",
    "\n",
    "def find_matching_prompts(prompts: List[Prompt], partial_prompt: str, limit: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find prompts that start with or contain the partial input\n",
    "    \"\"\"\n",
    "    if not partial_prompt:\n",
    "        return []\n",
    "    \n",
    "    partial_lower = partial_prompt.lower()\n",
    "    matches = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        prompt_text = prompt.metadata_info.prompt.lower()\n",
    "        if prompt_text.startswith(partial_lower):\n",
    "            matches.append((0, prompt))  # Higher priority for starts-with\n",
    "        elif partial_lower in prompt_text:\n",
    "            matches.append((1, prompt))  # Lower priority for contains\n",
    "    \n",
    "    # Sort by priority then by modification date\n",
    "    matches.sort(key=lambda x: (x[0], x[1].metadata_info.modified_at), reverse=True)\n",
    "    \n",
    "    return [p.metadata_info.prompt for _, p in matches[:limit]]\n",
    "\n",
    "# Add new endpoint\n",
    "@app.get(\"/prompts/suggest\", response_model=SuggestionResponse)\n",
    "async def suggest_prompts(\n",
    "    request: Request,\n",
    "    partial_prompt: str = Query(..., min_length=1, description=\"Partial prompt input for suggestions\"),\n",
    "    data_product: Optional[str] = Query(None, description=\"Filter suggestions by data product\"),\n",
    "    limit: int = Query(5, ge=1, le=10, description=\"Number of suggestions to return\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Get prompt suggestions based on partial user input\n",
    "    \n",
    "    Returns prompts that match or are similar to the partial input,\n",
    "    optionally filtered by data product.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract SSO token from request\n",
    "        sso_token = extract_sso_token(request)\n",
    "        \n",
    "        # Fetch all prompts (cached)\n",
    "        all_prompts = await fetch_all_prompts(sso_token)\n",
    "        \n",
    "        # Filter by data product if specified\n",
    "        if data_product:\n",
    "            all_prompts = filter_prompts_by_data_product(all_prompts, data_product)\n",
    "        \n",
    "        # Get exact matches first\n",
    "        exact_matches = find_matching_prompts(all_prompts, partial_prompt, limit)\n",
    "        \n",
    "        # If we don't have enough matches, add similar ones\n",
    "        if len(exact_matches) < limit:\n",
    "            similar_matches = find_similar_prompts(\n",
    "                all_prompts,\n",
    "                partial_prompt,\n",
    "                limit - len(exact_matches)\n",
    "            suggestions = exact_matches + similar_matches\n",
    "        else:\n",
    "            suggestions = exact_matches\n",
    "        \n",
    "        return {\"suggestions\": suggestions[:limit]}\n",
    "        \n",
    "    except HTTPException as he:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in suggest_prompts: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Ap\n",
    "\n",
    "from fastapi import FastAPI, Request, Query, HTTPException, Depends\n",
    "from fastapi.security import HTTPBearer\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from cachetools import TTLCache\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "PROMPT_LIBRARY_URL = os.getenv(\"PROMPT_LIBRARY_URL\", \"https://prompt-library-api.com/prompts\")\n",
    "CACHE_TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 300))  # 5 minutes\n",
    "MAX_LIMIT = int(os.getenv(\"MAX_SUGGESTION_LIMIT\", 20))\n",
    "API_TIMEOUT = int(os.getenv(\"API_TIMEOUT_SECONDS\", 10))\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Prompt Suggestion Service\",\n",
    "    description=\"API for fetching prompt suggestions by data product\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=None,\n",
    "    openapi_url=\"/openapi.json\"\n",
    ")\n",
    "\n",
    "# Security\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Cache setup\n",
    "prompt_cache = TTLCache(maxsize=1000, ttl=CACHE_TTL)\n",
    "\n",
    "# Models\n",
    "class PromptSuggestion(BaseModel):\n",
    "    id: str = Field(..., example=\"prompt_123\", description=\"Unique identifier for the prompt\")\n",
    "    text: str = Field(..., example=\"Analyze customer churn\", description=\"The prompt text\")\n",
    "    data_product: str = Field(..., example=\"Customer Analytics\", description=\"Name of the data product\")\n",
    "    last_modified: datetime = Field(..., example=\"2023-01-01T00:00:00Z\", description=\"When the prompt was last modified\")\n",
    "\n",
    "class SuggestionsResponse(BaseModel):\n",
    "    suggestions: List[PromptSuggestion]\n",
    "    data_product: str = Field(..., example=\"Customer Analytics\", description=\"The requested data product\")\n",
    "    count: int = Field(..., example=5, description=\"Number of suggestions returned\")\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    error: str\n",
    "    details: Optional[str] = None\n",
    "\n",
    "# Dependency for auth\n",
    "async def get_sso_token(request: Request) -> str:\n",
    "    auth_header = request.headers.get(\"x-auth\")\n",
    "    if not auth_header:\n",
    "        raise HTTPException(\n",
    "            status_code=401,\n",
    "            detail=\"Missing x-auth header\"\n",
    "        )\n",
    "    return auth_header.replace(\"Bearer \", \"\")\n",
    "\n",
    "# Helper functions\n",
    "def generate_auth_token() -> str:\n",
    "    \"\"\"Generate service auth token (implement your actual token generation)\"\"\"\n",
    "    return os.getenv(\"SERVICE_AUTH_TOKEN\", \"default-service-token\")\n",
    "\n",
    "def handle_errors(func):\n",
    "    @wraps(func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except HTTPException:\n",
    "            raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request to prompt library failed: {str(e)}\")\n",
    "            raise HTTPException(\n",
    "                status_code=502,\n",
    "                detail=\"Prompt library service unavailable\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=\"Internal server error\"\n",
    "            )\n",
    "    return wrapper\n",
    "\n",
    "# Service layer\n",
    "@handle_errors\n",
    "async def fetch_prompts(sso_token: str) -> List[dict]:\n",
    "    \"\"\"Fetch prompts from the library with caching\"\"\"\n",
    "    cache_key = \"all_prompts\"\n",
    "    if cache_key in prompt_cache:\n",
    "        logger.info(\"Returning cached prompts\")\n",
    "        return prompt_cache[cache_key]\n",
    "    \n",
    "    headers = {\n",
    "        \"x-auth\": f\"Bearer {sso_token}\",\n",
    "        \"Authorization\": f\"Bearer {generate_auth_token()}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        PROMPT_LIBRARY_URL,\n",
    "        headers=headers,\n",
    "        timeout=API_TIMEOUT\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    if not data.get(\"success\", False):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=data.get(\"message\", \"Invalid response from prompt library\")\n",
    "        )\n",
    "    \n",
    "    prompts = data.get(\"details\", {}).get(\"prompts\", [])\n",
    "    prompt_cache[cache_key] = prompts\n",
    "    return prompts\n",
    "\n",
    "def filter_and_sort_prompts(\n",
    "    prompts: List[dict],\n",
    "    data_product: str\n",
    ") -> List[dict]:\n",
    "    \"\"\"Filter prompts by data product and sort by modification date\"\"\"\n",
    "    filtered = []\n",
    "    for p in prompts:\n",
    "        metadata = p.get(\"metadata_info\", {})\n",
    "        if (data_product in metadata.get(\"data_product\", []) and\n",
    "            metadata.get(\"status\") == \"APPROVED\" and\n",
    "            metadata.get(\"is visible\", False)):\n",
    "            filtered.append(p)\n",
    "    \n",
    "    # Sort by modified_at descending\n",
    "    return sorted(\n",
    "        filtered,\n",
    "        key=lambda x: x.get(\"metadata_info\", {}).get(\"modified_at\", \"\"),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "# API Endpoint\n",
    "@app.get(\n",
    "    \"/suggestions\",\n",
    "    response_model=SuggestionsResponse,\n",
    "    responses={\n",
    "        400: {\"model\": ErrorResponse},\n",
    "        401: {\"model\": ErrorResponse},\n",
    "        500: {\"model\": ErrorResponse},\n",
    "        502: {\"model\": ErrorResponse}\n",
    "    },\n",
    "    summary=\"Get prompt suggestions\",\n",
    "    description=\"Fetch approved prompts for a specific data product, sorted by most recent first\"\n",
    ")\n",
    "async def get_suggestions(\n",
    "    data_product: str = Query(\n",
    "        ...,\n",
    "        min_length=1,\n",
    "        example=\"Customer Analytics\",\n",
    "        description=\"Name of the data product to filter by\"\n",
    "    ),\n",
    "    limit: int = Query(\n",
    "        5,\n",
    "        ge=1,\n",
    "        le=MAX_LIMIT,\n",
    "        example=5,\n",
    "        description=\"Maximum number of suggestions to return\"\n",
    "    ),\n",
    "    sso_token: str = Depends(get_sso_token)\n",
    ") -> SuggestionsResponse:\n",
    "    try:\n",
    "        # Fetch all prompts\n",
    "        all_prompts = await fetch_prompts(sso_token)\n",
    "        \n",
    "        # Filter and sort\n",
    "        filtered_prompts = filter_and_sort_prompts(all_prompts, data_product)\n",
    "        \n",
    "        # Format response\n",
    "        suggestions = [\n",
    "            PromptSuggestion(\n",
    "                id=p.get(\"id\", \"\"),\n",
    "                text=p.get(\"metadata_info\", {}).get(\"prompt\", \"\"),\n",
    "                data_product=data_product,\n",
    "                last_modified=p.get(\"metadata_info\", {}).get(\"modified_at\")\n",
    "            )\n",
    "            for p in filtered_prompts[:limit]\n",
    "        ]\n",
    "        \n",
    "        return SuggestionsResponse(\n",
    "            suggestions=suggestions,\n",
    "            data_product=data_product,\n",
    "            count=len(suggestions)\n",
    "            \n",
    "    except HTTPException as he:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_suggestions: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "# Health endpoints\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/ready\", include_in_schema=False)\n",
    "async def readiness_check():\n",
    "    try:\n",
    "        # Test a small cache operation\n",
    "        prompt_cache[\"healthcheck\"] = \"test\"\n",
    "        del prompt_cache[\"healthcheck\"]\n",
    "        return {\"status\": \"ready\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Readiness check failed: {str(e)}\")\n",
    "        raise HTTPException(status_code=503, detail=\"Service not ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a144a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already log\n",
    "\n",
    "from fastapi import FastAPI, Request, Query, HTTPException, Depends\n",
    "from fastapi.security import HTTPBearer\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from cachetools import TTLCache\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import os\n",
    "from .utils.logger import get_logger  # Import your existing logger utility\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Configuration\n",
    "PROMPT_LIBRARY_URL = os.getenv(\"PROMPT_LIBRARY_URL\", \"https://prompt-library-api.com/prompts\")\n",
    "CACHE_TTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 300))  # 5 minutes\n",
    "MAX_LIMIT = int(os.getenv(\"MAX_SUGGESTION_LIMIT\", 20))\n",
    "API_TIMEOUT = int(os.getenv(\"API_TIMEOUT_SECONDS\", 10))\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Prompt Suggestion Service\",\n",
    "    description=\"API for fetching prompt suggestions by data product\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=None,\n",
    "    openapi_url=\"/openapi.json\"\n",
    ")\n",
    "\n",
    "# Security\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Cache setup\n",
    "prompt_cache = TTLCache(maxsize=1000, ttl=CACHE_TTL)\n",
    "\n",
    "# Models\n",
    "class PromptSuggestion(BaseModel):\n",
    "    id: str = Field(..., example=\"prompt_123\", description=\"Unique identifier for the prompt\")\n",
    "    text: str = Field(..., example=\"Analyze customer churn\", description=\"The prompt text\")\n",
    "    data_product: str = Field(..., example=\"Customer Analytics\", description=\"Name of the data product\")\n",
    "    last_modified: datetime = Field(..., example=\"2023-01-01T00:00:00Z\", description=\"When the prompt was last modified\")\n",
    "\n",
    "class SuggestionsResponse(BaseModel):\n",
    "    suggestions: List[PromptSuggestion]\n",
    "    data_product: str = Field(..., example=\"Customer Analytics\", description=\"The requested data product\")\n",
    "    count: int = Field(..., example=5, description=\"Number of suggestions returned\")\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    error: str\n",
    "    details: Optional[str] = None\n",
    "\n",
    "# Dependency for auth\n",
    "async def get_sso_token(request: Request) -> str:\n",
    "    auth_header = request.headers.get(\"x-auth\")\n",
    "    if not auth_header:\n",
    "        logger.error(\"Missing x-auth header in request\")\n",
    "        raise HTTPException(\n",
    "            status_code=401,\n",
    "            detail=\"Missing x-auth header\"\n",
    "        )\n",
    "    return auth_header.replace(\"Bearer \", \"\")\n",
    "\n",
    "# Helper functions\n",
    "def generate_auth_token() -> str:\n",
    "    \"\"\"Generate service auth token\"\"\"\n",
    "    token = os.getenv(\"SERVICE_AUTH_TOKEN\")\n",
    "    if not token:\n",
    "        logger.warning(\"Using default service auth token - not recommended for production\")\n",
    "    return token or \"default-service-token\"\n",
    "\n",
    "def handle_errors(func):\n",
    "    @wraps(func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except HTTPException as he:\n",
    "            logger.warning(f\"HTTP Exception: {he.detail}\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request to prompt library failed: {str(e)}\", exc_info=True)\n",
    "            raise HTTPException(\n",
    "                status_code=502,\n",
    "                detail=\"Prompt library service unavailable\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=\"Internal server error\"\n",
    "            )\n",
    "    return wrapper\n",
    "\n",
    "# Service layer\n",
    "@handle_errors\n",
    "async def fetch_prompts(sso_token: str) -> List[dict]:\n",
    "    \"\"\"Fetch prompts from the library with caching\"\"\"\n",
    "    cache_key = \"all_prompts\"\n",
    "    if cache_key in prompt_cache:\n",
    "        logger.debug(\"Returning cached prompts\")\n",
    "        return prompt_cache[cache_key]\n",
    "    \n",
    "    logger.info(\"Fetching fresh prompts from library\")\n",
    "    headers = {\n",
    "        \"x-auth\": f\"Bearer {sso_token}\",\n",
    "        \"Authorization\": f\"Bearer {generate_auth_token()}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            PROMPT_LIBRARY_URL,\n",
    "            headers=headers,\n",
    "            timeout=API_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if not data.get(\"success\", False):\n",
    "            error_msg = data.get(\"message\", \"Invalid response from prompt library\")\n",
    "            logger.error(f\"Prompt library error: {error_msg}\")\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=error_msg\n",
    "            )\n",
    "        \n",
    "        prompts = data.get(\"details\", {}).get(\"prompts\", [])\n",
    "        prompt_cache[cache_key] = prompts\n",
    "        logger.info(f\"Fetched {len(prompts)} prompts, cached for {CACHE_TTL} seconds\")\n",
    "        return prompts\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch prompts: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def filter_and_sort_prompts(\n",
    "    prompts: List[dict],\n",
    "    data_product: str\n",
    ") -> List[dict]:\n",
    "    \"\"\"Filter prompts by data product and sort by modification date\"\"\"\n",
    "    filtered = []\n",
    "    for p in prompts:\n",
    "        metadata = p.get(\"metadata_info\", {})\n",
    "        if (data_product in metadata.get(\"data_product\", []) and\n",
    "            metadata.get(\"status\") == \"APPROVED\" and\n",
    "            metadata.get(\"is visible\", False)):\n",
    "            filtered.append(p)\n",
    "    \n",
    "    logger.debug(f\"Found {len(filtered)} prompts for data product {data_product}\")\n",
    "    \n",
    "    # Sort by modified_at descending\n",
    "    return sorted(\n",
    "        filtered,\n",
    "        key=lambda x: x.get(\"metadata_info\", {}).get(\"modified_at\", \"\"),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "# API Endpoint\n",
    "@app.get(\n",
    "    \"/suggestions\",\n",
    "    response_model=SuggestionsResponse,\n",
    "    responses={\n",
    "        400: {\"model\": ErrorResponse},\n",
    "        401: {\"model\": ErrorResponse},\n",
    "        500: {\"model\": ErrorResponse},\n",
    "        502: {\"model\": ErrorResponse}\n",
    "    },\n",
    "    summary=\"Get prompt suggestions\",\n",
    "    description=\"Fetch approved prompts for a specific data product, sorted by most recent first\"\n",
    ")\n",
    "async def get_suggestions(\n",
    "    data_product: str = Query(\n",
    "        ...,\n",
    "        min_length=1,\n",
    "        example=\"Customer Analytics\",\n",
    "        description=\"Name of the data product to filter by\"\n",
    "    ),\n",
    "    limit: int = Query(\n",
    "        5,\n",
    "        ge=1,\n",
    "        le=MAX_LIMIT,\n",
    "        example=5,\n",
    "        description=\"Maximum number of suggestions to return\"\n",
    "    ),\n",
    "    sso_token: str = Depends(get_sso_token)\n",
    ") -> SuggestionsResponse:\n",
    "    logger.info(f\"Getting suggestions for data product: {data_product}, limit: {limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch all prompts\n",
    "        all_prompts = await fetch_prompts(sso_token)\n",
    "        \n",
    "        # Filter and sort\n",
    "        filtered_prompts = filter_and_sort_prompts(all_prompts, data_product)\n",
    "        \n",
    "        # Format response\n",
    "        suggestions = [\n",
    "            PromptSuggestion(\n",
    "                id=p.get(\"id\", \"\"),\n",
    "                text=p.get(\"metadata_info\", {}).get(\"prompt\", \"\"),\n",
    "                data_product=data_product,\n",
    "                last_modified=p.get(\"metadata_info\", {}).get(\"modified_at\")\n",
    "            )\n",
    "            for p in filtered_prompts[:limit]\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Returning {len(suggestions)} suggestions for {data_product}\")\n",
    "        return SuggestionsResponse(\n",
    "            suggestions=suggestions,\n",
    "            data_product=data_product,\n",
    "            count=len(suggestions)\n",
    "        )\n",
    "            \n",
    "    except HTTPException as he:\n",
    "        logger.error(f\"HTTP error in get_suggestions: {he.detail}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in get_suggestions: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "# Health endpoints\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_check():\n",
    "    logger.debug(\"Health check endpoint called\")\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/ready\", include_in_schema=False)\n",
    "async def readiness_check():\n",
    "    logger.debug(\"Readiness check endpoint called\")\n",
    "    try:\n",
    "        # Test a small cache operation\n",
    "        prompt_cache[\"healthcheck\"] = \"test\"\n",
    "        del prompt_cache[\"healthcheck\"]\n",
    "        return {\"status\": \"ready\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Readiness check failed: {str(e)}\")\n",
    "        raise HTTPException(status_code=503, detail=\"Service not ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSO\n",
    "\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from typing import Optional\n",
    "import logging\n",
    "from .utils.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "security = HTTPBearer()\n",
    "\n",
    "async def extract_access_token(request: Request) -> str:\n",
    "    \"\"\"\n",
    "    Extract access token from either:\n",
    "    1. Authorization header (Bearer token) - for programmatic access\n",
    "    2. OR from x-auth header - for your manual testing workflow\n",
    "    \"\"\"\n",
    "    # First try to get from Authorization header (standard approach)\n",
    "    auth_header: Optional[HTTPAuthorizationCredentials] = await security(request)\n",
    "    \n",
    "    if auth_header:\n",
    "        logger.debug(\"Found token in Authorization header\")\n",
    "        return auth_header.credentials\n",
    "    \n",
    "    # Fallback to x-auth header (for your manual testing)\n",
    "    x_auth_header = request.headers.get(\"x-auth\")\n",
    "    if x_auth_header:\n",
    "        logger.debug(\"Found token in x-auth header\")\n",
    "        if x_auth_header.startswith(\"Bearer \"):\n",
    "            return x_auth_header[7:]\n",
    "        return x_auth_header\n",
    "    \n",
    "    logger.error(\"No access token found in headers\")\n",
    "    raise HTTPException(\n",
    "        status_code=401,\n",
    "        detail=\"Missing access token. Please provide either Authorization header or x-auth header\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/test-auth\")\n",
    "async def test_auth(request: Request):\n",
    "    \"\"\"Endpoint to test your token extraction\"\"\"\n",
    "    try:\n",
    "        token = await extract_access_token(request)\n",
    "        return {\n",
    "            \"message\": \"Token received successfully\",\n",
    "            \"token_length\": len(token),\n",
    "            \"note\": \"Never expose real tokens in responses in production!\"\n",
    "        }\n",
    "    except HTTPException as e:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Auth test failed: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.get(\"/prompts/suggestions\")\n",
    "async def get_suggestions(\n",
    "    request: Request,\n",
    "    data_product: str,\n",
    "    limit: int = 5\n",
    "):\n",
    "    \"\"\"Your actual endpoint\"\"\"\n",
    "    try:\n",
    "        # Get the access token from either header\n",
    "        access_token = await extract_access_token(request)\n",
    "        \n",
    "        # Now use this token to fetch prompts\n",
    "        # ... rest of your existing logic ...\n",
    "        \n",
    "        return {\"status\": \"success\", \"used_token\": access_token[-6:] + \"...\"}\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get suggestions: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "\n",
    "from fastapi import FastAPI, Query, HTTPException\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from cachetools import TTLCache\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from .utils.logger import get_logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Hardcoded configuration\n",
    "PROMPT_LIBRARY_URL = \"https://prompt-library-api.com/prompts\"\n",
    "HARDCODED_SSO_TOKEN = \"your_actual_sso_token_here\"  # DIRECTLY PUT YOUR TOKEN HERE\n",
    "SERVICE_AUTH_TOKEN = \"your_service_auth_token_here\"\n",
    "CACHE_TTL = 300  # 5 minutes\n",
    "MAX_LIMIT = 20\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Cache setup\n",
    "prompt_cache = TTLCache(maxsize=1000, ttl=CACHE_TTL)\n",
    "\n",
    "# Models\n",
    "class PromptSuggestion(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    data_product: str\n",
    "    last_modified: datetime\n",
    "\n",
    "class SuggestionsResponse(BaseModel):\n",
    "    suggestions: List[PromptSuggestion]\n",
    "    data_product: str\n",
    "    count: int\n",
    "\n",
    "def fetch_prompts() -> List[dict]:\n",
    "    \"\"\"Fetch prompts from the library with caching\"\"\"\n",
    "    cache_key = \"all_prompts\"\n",
    "    if cache_key in prompt_cache:\n",
    "        logger.debug(\"Returning cached prompts\")\n",
    "        return prompt_cache[cache_key]\n",
    "    \n",
    "    logger.info(\"Fetching fresh prompts from library\")\n",
    "    headers = {\n",
    "        \"x-auth\": f\"Bearer {HARDCODED_SSO_TOKEN}\",\n",
    "        \"Authorization\": f\"Bearer {SERVICE_AUTH_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(PROMPT_LIBRARY_URL, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if not data.get(\"success\", False):\n",
    "            error_msg = data.get(\"message\", \"Invalid response from prompt library\")\n",
    "            logger.error(f\"Prompt library error: {error_msg}\")\n",
    "            raise HTTPException(status_code=400, detail=error_msg)\n",
    "        \n",
    "        prompts = data.get(\"details\", {}).get(\"prompts\", [])\n",
    "        prompt_cache[cache_key] = prompts\n",
    "        logger.info(f\"Fetched {len(prompts)} prompts\")\n",
    "        return prompts\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch prompts: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to fetch prompts\")\n",
    "\n",
    "def filter_and_sort_prompts(prompts: List[dict], data_product: str) -> List[dict]:\n",
    "    \"\"\"Filter prompts by data product and sort by modification date\"\"\"\n",
    "    filtered = [\n",
    "        p for p in prompts\n",
    "        if (data_product in p.get(\"metadata_info\", {}).get(\"data_product\", [])\n",
    "            and p.get(\"metadata_info\", {}).get(\"status\") == \"APPROVED\"\n",
    "            and p.get(\"metadata_info\", {}).get(\"is visible\", False))\n",
    "    ]\n",
    "    \n",
    "    return sorted(\n",
    "        filtered,\n",
    "        key=lambda x: x.get(\"metadata_info\", {}).get(\"modified_at\", \"\"),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "@app.get(\"/suggestions\", response_model=SuggestionsResponse)\n",
    "async def get_suggestions(\n",
    "    data_product: str = Query(..., min_length=1),\n",
    "    limit: int = Query(5, ge=1, le=MAX_LIMIT)\n",
    "):\n",
    "    try:\n",
    "        # Fetch all prompts using hardcoded token\n",
    "        all_prompts = fetch_prompts()\n",
    "        \n",
    "        # Filter and sort\n",
    "        filtered_prompts = filter_and_sort_prompts(all_prompts, data_product)\n",
    "        \n",
    "        # Format response\n",
    "        suggestions = [\n",
    "            PromptSuggestion(\n",
    "                id=p.get(\"id\", \"\"),\n",
    "                text=p.get(\"metadata_info\", {}).get(\"prompt\", \"\"),\n",
    "                data_product=data_product,\n",
    "                last_modified=p.get(\"metadata_info\", {}).get(\"modified_at\")\n",
    "            )\n",
    "            for p in filtered_prompts[:limit]\n",
    "        ]\n",
    "        \n",
    "        return SuggestionsResponse(\n",
    "            suggestions=suggestions,\n",
    "            data_product=data_product,\n",
    "            count=len(suggestions)\n",
    "        )\n",
    "            \n",
    "    except HTTPException as he:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a716c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Query, HTTPException\n",
    "from typing import Optional, List\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "MAX_LIMIT = 20  # Maximum number of suggestions to return\n",
    "\n",
    "class PromptSuggestion(BaseModel):\n",
    "    prompt: str\n",
    "    id: str\n",
    "    data_products: List[str]\n",
    "    last_modified: datetime\n",
    "\n",
    "class SuggestionsResponse(BaseModel):\n",
    "    suggestions: List[PromptSuggestion]\n",
    "    data_products: Optional[List[str]] = None\n",
    "    keyword: Optional[str] = None\n",
    "    count: int\n",
    "\n",
    "def fetch_prompts() -> List[dict]:\n",
    "    \"\"\"\n",
    "    Mock function to fetch prompts - replace with your actual implementation\n",
    "    Returns list of prompts with structure:\n",
    "    [\n",
    "        {\n",
    "            \"id\": \"123\",\n",
    "            \"prompt\": \"prompt text\",\n",
    "            \"tags\": [\"tag1\", \"tag2\"],\n",
    "            \"metadata_info\": {\n",
    "                \"modified_at\": \"2023-01-01T00:00:00\"\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # TODO: Replace with your actual prompt fetching logic\n",
    "    return []\n",
    "\n",
    "def filter_and_sort_prompts(\n",
    "    prompts: List[dict],\n",
    "    tags: Optional[List[str]] = None,\n",
    "    keyword: Optional[str] = None,\n",
    "    sort_by: str = \"modified_at\"\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter prompts by data products (tags) and/or keyword, then sort by specified field\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt dictionaries\n",
    "        tags: Optional list of tags to filter by\n",
    "        keyword: Optional search term to filter prompts\n",
    "        sort_by: Field to sort by (default: \"modified_at\")\n",
    "    \n",
    "    Returns:\n",
    "        Filtered and sorted list of prompts\n",
    "    \"\"\"\n",
    "    filtered = prompts\n",
    "    \n",
    "    # Filter by tags if provided\n",
    "    if tags:\n",
    "        filtered = [\n",
    "            p for p in filtered\n",
    "            if any(tag.strip().lower() in [t.lower() for t in p.get('tags', [])]\n",
    "                  for tag in tags if tag.strip())\n",
    "        ]\n",
    "    \n",
    "    # Filter by keyword if provided\n",
    "    if keyword:\n",
    "        keyword_lower = keyword.lower()\n",
    "        filtered = [\n",
    "            p for p in filtered\n",
    "            if keyword_lower in p.get(\"prompt\", \"\").lower()\n",
    "        ]\n",
    "    \n",
    "    # Sort results\n",
    "    if sort_by == \"modified_at\":\n",
    "        return sorted(\n",
    "            filtered,\n",
    "            key=lambda x: x.get(\"metadata_info\", {}).get(\"modified_at\", \"\"),\n",
    "            reverse=True\n",
    "        )\n",
    "    else:\n",
    "        return sorted(filtered, key=lambda x: x.get(sort_by, \"\"), reverse=True)\n",
    "\n",
    "@app.get(\"/suggestions\", response_model=SuggestionsResponse)\n",
    "async def get_suggestions(\n",
    "    keyword: Optional[str] = Query(None, min_length=1, description=\"Search term to filter prompts\"),\n",
    "    data_products: Optional[List[str]] = Query(\n",
    "        None,\n",
    "        description=\"List of data products to filter by\",\n",
    "        alias=\"data_product\"  # Allows both 'data_product' and 'data_products' in query\n",
    "    ),\n",
    "    limit: int = Query(5, ge=1, le=MAX_LIMIT, description=\"Maximum number of suggestions to return\")\n",
    "):\n",
    "    try:\n",
    "        # Fetch all prompts\n",
    "        all_prompts = fetch_prompts()\n",
    "        \n",
    "        # Filter and sort prompts\n",
    "        filtered_prompts = filter_and_sort_prompts(\n",
    "            prompts=all_prompts,\n",
    "            tags=data_products,\n",
    "            keyword=keyword\n",
    "        )\n",
    "        \n",
    "        # Format response\n",
    "        suggestions = [\n",
    "            PromptSuggestion(\n",
    "                prompt=p.get(\"prompt\", \"\"),\n",
    "                id=p.get(\"id\", \"\"),\n",
    "                data_products=p.get(\"tags\", []),\n",
    "                last_modified=p.get(\"metadata_info\", {}).get(\"modified_at\")\n",
    "            )\n",
    "            for p in filtered_prompts[:limit]\n",
    "        ]\n",
    "        \n",
    "        return SuggestionsResponse(\n",
    "            suggestions=suggestions,\n",
    "            data_products=data_products,\n",
    "            keyword=keyword,\n",
    "            count=len(suggestions)\n",
    "        )\n",
    "        \n",
    "    except HTTPException as he:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d33725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c79e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa4cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
